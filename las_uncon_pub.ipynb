{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing predictions of patient conveyance using emergency call handler free text notes for unconscious and fainting incidents reported to the London Ambulance Service"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load main packages and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "\n",
    "# ↓Required otherwise display of text strings truncated\n",
    "pd.set_option(\"display.max_colwidth\", 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data set\n",
    "las18_p31 = pd.read_csv(\n",
    "    \"~/msc-las/data/nlp_data_extract_010118-311218.csv\", encoding=\"ISO-8859–1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outcome measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Fill nan values in vehicle dispatched column\n",
    "las18_p31[\"vehiclesarrivedx\"] = las18_p31[\"vehiclesarrived\"].fillna(0)\n",
    "\n",
    "# Create binary dispatch attribute for if one or more vehicles was dispatched\n",
    "las18_p31[\"dispatched\"] = np.where(las18_p31[\"vehiclesarrivedx\"] == 0, 0, 1)\n",
    "\n",
    "# Create outcome attribute covering the four patient outcomes\n",
    "las18_p31[\"conveyed\"] = las18_p31[\"conveyed\"].fillna(0)\n",
    "las18_p31[\"conveyed_ed\"] = las18_p31[\"conveyed_ed\"].fillna(0)\n",
    "las18_p31[\"conveyed_other\"] = las18_p31[\"conveyed\"] - las18_p31[\"conveyed_ed\"]\n",
    "las18_p31[\"outcome_num\"] = las18_p31[\"dispatched\"] + \\\n",
    "    las18_p31[\"conveyed\"] + las18_p31[\"conveyed_ed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Tidy ampds attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove ampds codes incorrectly read in log form\n",
    "las18_p31[\"ampdscode\"] = las18_p31[\"ampdscode\"].str.replace(\n",
    "    \"3.10E+0\", \"31E\", regex=False)\n",
    "\n",
    "# Create combined ampds_full description/code attribute for easy reading in charts\n",
    "las18_p31[\"ampds_full\"] = las18_p31[[\"ampdscode\", \"description\"]].apply(\n",
    "    lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidying ampds_full descriptions consistent with \n",
    "#https://wiki.radioreference.com/index.php/Priority_Dispatch_Codes\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31D2 Unconscious or Fainting - Effective Breathing\", \n",
    "    \"31D2 Unconscious - Effective Breathing\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31D3 Unconscious or Fainting - Not Alert\", \"31D3 Not Alert\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31D1 Unconscious  Agonal / Ineffective Breathing\", \n",
    "    \"31D1 Unconscious - Agonal / Ineffective Breathing\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31D4 Unconscious or Fainting - Changing Colour\",\n",
    "    \"31D4 Changing Colour\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31D4 Unconscious or Fainting - Changing Colour\", \n",
    "    \"31D4 Changing Colour\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31E2 Unconscious or Fainting - Ineffective Breathing\", \n",
    "    \"31E2 Ineffective Breathing\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31C1 Unconscious or Fainting - Alert with Abnormal Breathing\", \n",
    "    \"31C1 Alert with Abnormal Breathing\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31C1 Unconscious or Fainting - Alert with Abnormal Breathing\", \n",
    "    \"31C1 Alert with Abnormal Breathing\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31E1 Unconscious or Fainting (near) Echo Override\", \n",
    "    \"31E1 Echo Override\", regex=False)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31D0 Unconscious or Fainting (near) Delta Override\", \n",
    "    \"31D0 Delta Override\", regex=False)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31C0 Unconscious or (Near) Fainting Charlie Override\", \n",
    "    \"31C0 Charlie Override\", regex=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String length of problem description\n",
    "las18_p31[\"string_len\"] = las18_p31.problemdescription.str.len()\n",
    "\n",
    "# Strip all numbers from problem description to remove dates of birht\n",
    "las18_p31[\"prob_desc\"] = las18_p31.problemdescription.apply(\n",
    "    lambda x: re.sub(r\"[0-9]+\", \"\", str(x)))\n",
    "\n",
    "# Drop raw problem descrption\n",
    "las18_p31 = las18_p31.drop(\"problemdescription\", axis=1)\n",
    "\n",
    "# Lower case the problem description\n",
    "las18_p31[\"prob_desc\"] = las18_p31[\"prob_desc\"].astype(\n",
    "    str).apply(lambda x: x.lower())\n",
    "\n",
    "# New attribute with punctucation removed\n",
    "las18_p31[\"prob_desc_no_punc\"] = las18_p31.prob_desc.apply(\n",
    "    lambda x: x.translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
    "\n",
    "# String length after removing punctuation and numbers\n",
    "las18_p31[\"string_len_p\"] = las18_p31[\"prob_desc_no_punc\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize text\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "def lemma(text):\n",
    "    doc = nlp(text)\n",
    "    # Extract the lemma for each token and join\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "\n",
    "las18_p31[\"prob_desc_lemmat\"] = las18_p31[\"prob_desc\"].apply(\n",
    "    lambda x: lemma(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save wrangled data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pickle with processed data\n",
    "import os\n",
    "\n",
    "las18_p31.to_pickle(os.path.join(\"pickle\",\"las18_p31.pickle\"))\n",
    "\n",
    "del(las18_p31)\n",
    "las18_p31 = pd.read_pickle(os.path.join(\"pickle\",\n",
    "                                              \"las18_p31.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Split and filter data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified sample (to preserve ratio of outcomes after dropping\n",
    "# no-dispatch calls - relevant for complete analysis undertaken in MSc\n",
    "# Project but not for published paper)\n",
    "\n",
    "amb_train_0, amb_test_0 = train_test_split(\n",
    "    las18_p31, test_size=0.20, random_state=100, \n",
    "    stratify=las18_p31[\"outcome_num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop if no face-to-face response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only instances where ambulance was dispatched\n",
    "amb_train = amb_train_0.loc[amb_train_0.dispatched != 0]\n",
    "amb_train = copy.deepcopy(amb_train) # Create as new object\n",
    "\n",
    "amb_test = amb_test_0.loc[amb_test_0.dispatched != 0]\n",
    "amb_test = copy.deepcopy(amb_test)\n",
    "\n",
    "no_disp = len(amb_train_0) - len(amb_train)\n",
    "str(\"Number of calls with no dispatch: \") + str(no_disp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate summary table for report\n",
    "\n",
    "def desc_stats1(vab, df, partition):\n",
    "    x = df.groupby(by=[vab, \"conveyed\"]).count()[\"incidentid\"].unstack()\n",
    "    x[\"n\"] = x.sum(axis=1).astype(int) # Total (conveyed + not conveyed) \n",
    "    x[\"n/N (%)\"] = x.n/x.n.sum()*100  # % of total\n",
    "    x[\"n/N (%)\"] = x[\"n/N (%)\"].round(1)\n",
    "    x[\"conveyed/n (%)\"] = x[1.0]/x[\"n\"]*100 # Conveyance rate\n",
    "    x[\"conveyed/n (%)\"] = x[\"conveyed/n (%)\"].round(1)\n",
    "    x = x.rename(columns={1.0: \"conveyed\"}) # Number conveyed\n",
    "    x.index.name = None\n",
    "    x = x.drop(columns=0.0) # Remove not conveyed number\n",
    "    x = x[[\"n\", \"n/N (%)\", \"conveyed\", \n",
    "       \"conveyed/n (%)\"]].rename_axis(\"\", axis=\"columns\") # Define column order\n",
    "    x = pd.concat([x], keys=[partition], names=[\"\"], axis=1)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPDS code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Number of MPDS P31 determinant codes\n",
    "len(amb_train.ampds_full.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_stats1(\"ampds_full\", amb_train, \"Training set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Average token length (excluding punctuation) TRAINING SET\n",
    "\n",
    "def av_length(df):\n",
    "    free_text = df.prob_desc.to_string(index=False)\n",
    "    free_text = nltk.word_tokenize(free_text)\n",
    "\n",
    "    all_lengths = []\n",
    "    num_of_strings = len(free_text)\n",
    "\n",
    "    for item in free_text:\n",
    "        if item not in string.punctuation:\n",
    "            string_size = len(item)\n",
    "            all_lengths.append(string_size)\n",
    "            total_size = sum(all_lengths)\n",
    "    ave_size = float(total_size) / float(num_of_strings)\n",
    "    \n",
    "    return ave_size, len(set(free_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_text = {\"Average string length\": \n",
    "                amb_train[\"string_len\"].mean().round(1),\n",
    "                \"Average string length (not conveyed)\":\n",
    "                amb_train.loc[amb_train.conveyed ==0 ][\"string_len\"].mean().round(1),\n",
    "               \"ave_size_unique\": av_length(amb_train)}\n",
    "\n",
    "pd.DataFrame({\"Training set\":training_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from wordcloud import WordCloud, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set an empty list to use as stopwords in Word Cloud (don't want stopwords \n",
    "# removed)\n",
    "stopwords = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All text as one string\n",
    "all_text = amb_train.prob_desc.to_string(index=False)\n",
    "\n",
    "# Make Word Cloud\n",
    "wordcloud = WordCloud(background_color=\"white\",\n",
    "                      width=1009, height=750, stopwords = stopwords,\n",
    "                      collocations=False).generate(all_text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "wordcloud.to_file(\"figures_new/cloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All text as one string\n",
    "\n",
    "# Make Word Cloud\n",
    "wordcloud = WordCloud(background_color=\"white\",\n",
    "                      width=1009, height=750, stopwords = stopwords,\n",
    "                      collocations=True).generate(all_text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "wordcloud.to_file(\"figures_new/cloud2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Unigram, bigram and trigram plots excluding punctuation\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "fig.subplots_adjust(wspace=1)\n",
    "\n",
    "free_text = amb_train.prob_desc_no_punc.to_string(index=False)\n",
    "free_text = nltk.word_tokenize(free_text)\n",
    "\n",
    "unigrams = nltk.FreqDist(free_text).most_common(20) # Only top 20\n",
    "unigrams = list(zip(*unigrams)) # Separate list of pairs in to list of 2 tuples\n",
    "unigrams_bg = list(unigrams[0])\n",
    "unigrams_bg.reverse() # Needed to get top bar as highest \n",
    "unigrams_freq = list(unigrams[1])\n",
    "unigrams_freq.reverse() # Needed to get top bar as highest\n",
    "ax1 = fig.add_subplot(1, 3, 1) # Left plot\n",
    "ax1.barh(unigrams_bg, unigrams_freq, color=\"green\")\n",
    "ax1.set_title(\"Unigrams\")\n",
    "\n",
    "# As above for bigrams\n",
    "bigrams = nltk.FreqDist(nltk.bigrams(free_text)).most_common(20)\n",
    "bigrams_bg = [\" \".join(pair[0]) for pair in bigrams]\n",
    "bigrams_bg.reverse()\n",
    "bigrams_freq = list(list(zip(*bigrams))[1])\n",
    "bigrams_freq.reverse()\n",
    "ax2 = fig.add_subplot(1, 3, 2)\n",
    "ax2.barh(bigrams_bg, bigrams_freq, color=\"green\")\n",
    "ax2.set_title(\"Bigrams\")\n",
    "\n",
    "# As above for trigrams\n",
    "trigrams = nltk.FreqDist(nltk.trigrams(free_text)).most_common(25)\n",
    "trigrams_bg = [\" \".join(pair[0]) for pair in trigrams]\n",
    "trigrams_bg.reverse()\n",
    "trigrams_freq = list(list(zip(*trigrams))[1])\n",
    "trigrams_freq.reverse()\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "ax3.barh(trigrams_bg, trigrams_freq, color=\"green\")\n",
    "ax3.set_title(\"Trigrams\")\n",
    "fig.savefig(\"figures_new/ngrams.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amb_train[\"string_len\"].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X axis labels\n",
    "amb_train[\"string_len_band\"] = pd.cut(las18_p31.string_len, \n",
    "                                      bins=np.arange(0, 250, 10).tolist())\n",
    "\n",
    "# String length bands frequency and conveyance\n",
    "age_agg = pd.DataFrame(amb_train.groupby(\n",
    "    by=[\"string_len_band\", \"conveyed\"]).count()[\"incidentid\"]).unstack()\n",
    "age_agg.columns = age_agg.columns.droplevel(0)\n",
    "age_agg[\"Incident volume\"] = age_agg[0.0] + age_agg[1.0]\n",
    "age_agg[\"% conveyed\"] = age_agg[1.0]*100/ age_agg[\"Incident volume\"]\n",
    "\n",
    "bands = pd.IntervalIndex(age_agg.reset_index()[\"string_len_band\"]).mid\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.set_xlabel('string_len')\n",
    "ax1.grid(False)\n",
    "ax1.set_ylabel('Incident volume', color=\"darkgreen\")\n",
    "ax1.bar(bands, age_agg[\"Incident volume\"], color=\"darkgreen\", width=4)\n",
    "ax1.tick_params(axis='y', labelcolor=\"darkgreen\")\n",
    "ax1.set_xlim(0,110)\n",
    "ax1.set_xticks(np.arange(0,240,20))\n",
    "ax1.set_ylim(0,10000)\n",
    "ax1.set_yticks(np.arange(0,10000,1000))\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "ax2.set_ylabel('% conveyed', color=\"darkblue\")\n",
    "ax2.scatter(bands, age_agg[\"% conveyed\"], color=\"darkblue\")\n",
    "ax2.plot(bands, age_agg[\"% conveyed\"], color=\"darkblue\")\n",
    "ax2.tick_params(axis='y', labelcolor=\"darkblue\")\n",
    "ax2.set_ylim(0,100)\n",
    "ax2.set_yticks(np.arange(0,110,20))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"figures_new/str_len.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model classifers and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer, LabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, cross_validate \n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Create specificity and true negative scorers\n",
    "specificity = make_scorer(metrics.recall_score, pos_label=0)\n",
    "true_negative = make_scorer(metrics.precision_score, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main classifiers\n",
    "classifiers1 = {\"RF\": RandomForestClassifier(n_estimators=100, \n",
    "                                             random_state=0, \n",
    "                                             min_samples_leaf=50, n_jobs=-1),\n",
    "                \"GBM\": GradientBoostingClassifier(\n",
    "    min_samples_leaf=50, max_depth=100 ), \n",
    "                \"RF_bal\": RandomForestClassifier(n_estimators=100, \n",
    "                                                 random_state=0, \n",
    "                                                 class_weight=\"balanced\", \n",
    "                                                 min_samples_leaf=50, n_jobs=-1)}\n",
    "\n",
    "# Define label\n",
    "y_varb = \"conveyed\"\n",
    "\n",
    "# Define scoring metrics NB recall = sensitivity\n",
    "scoring = {\"accuracy\": \"accuracy\", \"precision\": \"precision\",\n",
    "           \"roc_auc\": \"roc_auc\",\n",
    "           \"sensitivity\": \"recall\", \"specificity\": specificity,\n",
    "           \"true_negative\": true_negative}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function to incorporate pipelines and fitting of models\n",
    "\n",
    "def process_results_1(key, pipeline, variables1, results, svd):\n",
    "    # Fit model\n",
    "    pipeline.fit(amb_train[variables1].values,\n",
    "                 amb_train[y_varb].values)\n",
    "\n",
    "    # Feature importances\n",
    "    if svd:\n",
    "        feature_names = np.nan\n",
    "        imps = np.nan\n",
    "        estimator = np.nan\n",
    "    else:\n",
    "        if key == \"LSV_bal\": # LSV has coefs rather than feature imps\n",
    "            feature_names = pipeline.named_steps[\"process\"].named_steps[\"final\"].get_feature_names(\n",
    "            )\n",
    "            zit = pd.Series(pipeline.named_steps[\"classifier\"].coef_[\n",
    "                            :, 0], index=feature_names).sort_values(ascending=False)\n",
    "            imps = zit[0:20].round(2).to_dict()\n",
    "            estimator = np.nan\n",
    "        else:\n",
    "            feature_names = pipeline.named_steps[\"process\"].named_steps[\"final\"].get_feature_names(\n",
    "            )\n",
    "            imps = pd.Series(pipeline.named_steps[\"classifier\"].feature_importances_, index=feature_names).sort_values(\n",
    "                ascending=False)\n",
    "            imps = imps[0:10].round(2).to_dict()\n",
    "\n",
    "            # for Decision tree viz\n",
    "            estimator = pipeline.named_steps[\"classifier\"].estimators_[10]\n",
    "\n",
    "    return feature_names, imps, estimator\n",
    "\n",
    "# Function to incorporate cross validation and results processing\n",
    "def process_results_2(key, pipeline, variables1, variables2, results,\n",
    "                      model, variant, feature_names, imps, estimator):\n",
    "    \n",
    "    # Cross validate model\n",
    "    output = cross_validate(pipeline, amb_train[variables1].values,\n",
    "                            amb_train[y_varb].values,\n",
    "                            scoring=scoring, cv=5, return_train_score=False,\n",
    "                            return_estimator=True)\n",
    "\n",
    "    mean = output[\"test_roc_auc\"].mean()\n",
    "\n",
    "    # Predict on test set\n",
    "    pipeline.predict(amb_test[variables1].values)\n",
    "    y_pred = pipeline.predict(amb_test[variables1].values)\n",
    "    y_pred_proba = pipeline.predict_proba(amb_test[variables1].values)\n",
    "\n",
    "    # Metrics\n",
    "    ex_sensitivity = metrics.recall_score(amb_test[y_varb].values, y_pred).round(3)\n",
    "    ex_specificity = specificity(\n",
    "        pipeline, amb_test[variables1].values, amb_test[y_varb].values).round(3)\n",
    "\n",
    "    ex_precision = metrics.precision_score(\n",
    "        amb_test[y_varb].values, y_pred).round(3)\n",
    "    ex_true_negative = true_negative(\n",
    "        pipeline, amb_test[variables1].values, amb_test[y_varb].values).round(3)\n",
    "    ex_roc_auc = metrics.roc_auc_score(\n",
    "        amb_test[y_varb].values, y_pred_proba[:, 1]).round(3)\n",
    "\n",
    "    # IDs of correctly classified\n",
    "    test_predict_bool = y_pred == 0\n",
    "    not_conv_id = amb_test[\"incidentid\"].loc[(\n",
    "        test_predict_bool) & (amb_test.conveyed == 0)].values\n",
    "\n",
    "    # DeLong CI\n",
    "    auc, auc_cov = delong_roc_variance(\n",
    "        amb_test[y_varb].values,\n",
    "        y_pred_proba[:, 1])\n",
    "\n",
    "    alpha = 0.95\n",
    "\n",
    "    auc_std = np.sqrt(auc_cov)\n",
    "    lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "    ci = stats.norm.ppf(lower_upper_q, loc=auc, scale=auc_std)\n",
    "    ci[ci > 1] = 1\n",
    "\n",
    "    roc_auc_ci = f\" {ex_roc_auc.round(3)} ( {ci[0].round(3)} - {ci[1].round(3)} )\"\n",
    "    \n",
    "    # List of dictionaries for results tables\n",
    "    results.append({\"Model\": str(model) + str(key), \"Classifier\": key, \"CV sensitivity\": output[\"test_sensitivity\"].mean().round(3),\n",
    "                    \"CV specificity\": output[\"test_specificity\"].mean().round(3),\n",
    "                    \"CV precision\": output[\"test_precision\"].mean().round(3),\n",
    "                    \"CV true negative rate\": output[\"test_true_negative\"].mean().round(3),\n",
    "                    \"CV roc auc\": mean.round(3),\n",
    "                    \"Feature Imps\": imps, \"Features\": \", \".join(\n",
    "        variables2), \"Variant\": variant, \"Test precision\": ex_precision,\n",
    "                    \"Test sensitivity\": ex_sensitivity, \"Test specificity\": ex_specificity,\n",
    "                    \"Test true negative rate\": ex_true_negative,\n",
    "                    \"Test roc auc\": ex_roc_auc, \"Test roc auc (CI)\": roc_auc_ci,\n",
    "                    \"Test y_pred_proba\": y_pred_proba, \"Test y_pred\": y_pred,\n",
    "                    \"Estimator\": estimator,  \"Feature Names\": feature_names,\n",
    "                    \"Test not conveyed IDs\": not_conv_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model run functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Run model categorical variable only\n",
    "def modelrun_cat(variables, model, variant, classifiers=classifiers1,\n",
    "                 svd=None, clustering=None):\n",
    "    results = []  # list to contain outputs from each classifier\n",
    "    variables1 = variables\n",
    "    variables2 = variables\n",
    "    if svd:\n",
    "        categories_pipe = Pipeline(steps=[(\"imputer\",\n",
    "                                           SimpleImputer(strategy=\"constant\",\n",
    "                                                         fill_value=\"missing\")), (\n",
    "            \"final\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "                                          (\"truncsvd\", TruncatedSVD(svd))])\n",
    "\n",
    "    else:\n",
    "        categories_pipe = Pipeline(steps=[(\n",
    "            \"imputer\", SimpleImputer(\n",
    "            strategy=\"constant\",\n",
    "                fill_value=\"missing\")), (\"final\",\n",
    "                                           OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "    for key, value in classifiers.items():  # for each classifier\n",
    "        pipeline = Pipeline(\n",
    "            [(\"process\", categories_pipe,), (\"classifier\", classifiers[key]), ])\n",
    "\n",
    "        ret = process_results_1(key, pipeline, variables1, results, svd)\n",
    "        feature_names = ret[0]\n",
    "        imps = ret[1]\n",
    "        estimator = ret[2]\n",
    "\n",
    "        process_results_2(key, pipeline, variables1, variables2,\n",
    "                          results, model, variant, feature_names, imps, estimator)\n",
    "    return pd.DataFrame(results).set_index([\"Model\", \"Features\", \"Variant\", \"Classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Run model with count vectorizer on text data\n",
    "def modelrun_text(text, model, variant, classifiers=classifiers1, svd=None, min_df=10, ngram_range=(1, 1)):\n",
    "    results = []  # list to contain outputs from each classifier\n",
    "    variables1 = text\n",
    "    variables2 = []\n",
    "    variables2.append(text)\n",
    "    if svd:\n",
    "        text_pipe = Pipeline([(\n",
    "            \"final\", CountVectorizer(analyzer=\"word\",\n",
    "                                     token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\",\n",
    "                                                        ngram_range=ngram_range,\n",
    "                                     min_df=min_df)),\n",
    "            (\"truncsvd\", TruncatedSVD(svd))])\n",
    "\n",
    "    else:\n",
    "        text_pipe = Pipeline([(\n",
    "            \"final\", CountVectorizer(analyzer=\"word\",\n",
    "                                     token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\",\n",
    "                                                        ngram_range=ngram_range, min_df=min_df))])\n",
    "\n",
    "    for key, value in classifiers.items():  # for each classifier\n",
    "        pipeline = Pipeline(\n",
    "            [(\"process\", text_pipe,), (\"classifier\", classifiers[key]), ])\n",
    "\n",
    "        ret = process_results_1(key, pipeline, variables1, results, svd)\n",
    "        feature_names = ret[0]\n",
    "        imps = ret[1]\n",
    "        estimator = ret[2]\n",
    "\n",
    "        process_results_2(key, pipeline, variables1, variables2,\n",
    "                          results, model, variant, feature_names, imps, estimator)\n",
    "    return pd.DataFrame(results).set_index([\"Model\", \"Features\", \"Variant\", \"Classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Run model with TFIDF vectorizer on text data\n",
    "def modelrun_text_tfidf(text, model, variant, classifiers=classifiers1,\n",
    "                        svd=None, min_df=10, ngram_range=(1, 1)):\n",
    "    results = []  # list to contain outputs from each classifier\n",
    "    variables1 = text\n",
    "    variables2 = []\n",
    "    variables2.append(text)\n",
    "    if svd:\n",
    "        text_pipe = Pipeline([(\n",
    "            \"final\", ), (\"truncsvd\", TruncatedSVD(svd))])\n",
    "\n",
    "    else:\n",
    "        text_pipe = Pipeline([(\"final\", TfidfVectorizer(\n",
    "            analyzer=\"word\", token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\", ngram_range=ngram_range, min_df=min_df))])\n",
    "\n",
    "    for key, value in classifiers.items():  # for each classifier\n",
    "        pipeline = Pipeline(\n",
    "            [(\"process\", text_pipe,), (\"classifier\", classifiers[key]), ])\n",
    "\n",
    "        ret = process_results_1(key, pipeline, variables1, results, svd)\n",
    "        feature_names = ret[0]\n",
    "        imps = ret[1]\n",
    "        estimator = ret[2]\n",
    "\n",
    "        process_results_2(key, pipeline, variables1, variables2,\n",
    "                          results, model, variant, feature_names, imps, estimator)\n",
    "    return pd.DataFrame(results).set_index([\"Model\", \"Features\", \"Variant\", \"Classifier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required to allow extraction of feature names from Pipeline in Column Transformer:\n",
    "https://stackoverflow.com/questions/48005889/get-features-from-sklearn-feature-union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class myPipeline(Pipeline):\n",
    "    def get_feature_names(self):\n",
    "        for name, step in self.steps:\n",
    "            if isinstance(step, CountVectorizer):\n",
    "                return step.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Run model for mixed feature case\n",
    "def modelrun_all_count(model, variant, classifiers=classifiers1, text=None,\n",
    "                       others=None, loc_clusters=None):\n",
    "    variables = []  # list to contain all variables\n",
    "    results = []  # list to contain outputs from each classifier\n",
    "    num_vars = {}  # dict to contain numerical variable positions\n",
    "    cat_vars = {}  # dict to contain cat variables positions\n",
    "    listy = []  # list to contain all elements of main pipeline\n",
    "\n",
    "    numerical_pipe = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer()), (\"scale\", StandardScaler())])\n",
    "\n",
    "    text_pipe = myPipeline([(\"vect\", CountVectorizer(\n",
    "        analyzer=\"word\", token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\")), ])\n",
    "\n",
    "    listy.append((\"text\", text_pipe, 0))\n",
    "    variables.append(text)\n",
    "\n",
    "    variables = variables + others\n",
    "    for i in range(len(others)):\n",
    "        if amb_train[others[i]].dtype == \"object\":\n",
    "            cat_vars[others[i]] = i + 1\n",
    "        else:\n",
    "            num_vars[others[i]] = i + 1\n",
    "\n",
    "    categories_pipe = myPipeline(steps=[(\"imputer\", SimpleImputer(\n",
    "        strategy=\"constant\", fill_value=\"missing\")),\n",
    "                                        (\"onehot\", \n",
    "                                         OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "    if cat_vars:  # If there are categorical features\n",
    "        listy.append((\"categories\", categories_pipe, [*cat_vars.values()]))\n",
    "\n",
    "    variables1 = variables # Needed to be consistent with other functions\n",
    "    variables2 = variables\n",
    "\n",
    "    for key, value in classifiers.items():  # For each classifier\n",
    "        pipeline = myPipeline(\n",
    "            [(\"union\", ColumnTransformer(\n",
    "                listy, transformer_weights={\"text\": 1, \"num\": 1,\n",
    "                                            \"categories\": 1, \"clust\": 1},)),\n",
    "             (\"classifier\", classifiers[key]), ])\n",
    "\n",
    "        # Fit model\n",
    "        pipeline.fit(amb_train[variables].values,\n",
    "                     amb_train[y_varb].values)\n",
    "\n",
    "        feature_names1 = pipeline.named_steps[\"union\"].transformers_[\n",
    "            0][1].named_steps[\"vect\"].get_feature_names()\n",
    "\n",
    "        feature_names2 = pipeline.named_steps[\"union\"].transformers_[\n",
    "            1][1].named_steps[\"onehot\"].get_feature_names()\n",
    "\n",
    "        feature_names = feature_names1 + feature_names2.tolist()\n",
    "        feature_names = feature_names + list(num_vars.keys())\n",
    "        \n",
    "        imps = pd.Series(pipeline.named_steps[\"classifier\"].feature_importances_,\n",
    "                         index=feature_names).sort_values(ascending=False)\n",
    "        imps = imps[0:20].round(2).to_dict()\n",
    "\n",
    "        estimator = pipeline.named_steps[\"classifier\"].estimators_[90]\n",
    "\n",
    "        process_results_2(key, pipeline, variables1, variables2,\n",
    "                          results, model, variant, feature_names, imps, estimator)\n",
    "    return pd.DataFrame(results).set_index([\"Model\", \n",
    "                                            \"Features\", \"Variant\", \"Classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeLong method code taken from:\n",
    "    https://github.com/yandexdataschool/roc_comparison/blob/master/compare_auc_delong_xu.py\n",
    "\n",
    "and\n",
    "        \n",
    "https://stackoverflow.com/questions/19124239/scikit-learn-roc-curve-with-confidence-intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     29,
     53,
     98,
     126
    ]
   },
   "outputs": [],
   "source": [
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and (Z[j] == Z[i]).all():\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight=None):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count  # ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    sample_weight = None\n",
    "    order, label_1_count = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
    "    assert len(\n",
    "        aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "\n",
    "def delong_roc_test(ground_truth, predictions_one, predictions_two):\n",
    "    \"\"\"\n",
    "    Computes log(p-value) for hypothesis that two ROC AUCs are different\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions_one: predictions of the first model,\n",
    "          np.array of floats of the probability of being class 1\n",
    "       predictions_two: predictions of the second model,\n",
    "          np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    sample_weight = None\n",
    "    order, label_1_count = compute_ground_truth_statistics(ground_truth)\n",
    "    predictions_sorted_transposed = np.vstack(\n",
    "        (predictions_one, predictions_two))[:, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
    "    return calc_pvalue(aucs, delongcov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = modelrun_cat(\n",
    "    variables=[\"ampdscode\"], model=\"1\", variant=\"-\")\n",
    "results.to_csv(\"results.csv\")\n",
    "results.to_pickle(os.path.join(\"pickle\", \"results.pickle\"))\n",
    "results = pd.read_pickle(\n",
    "    os.path.join(\"pickle\", \"results.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(next_mod, results):\n",
    "    results = results.append(next_mod)\n",
    "    results.to_csv(\"results.csv\")\n",
    "    results.to_pickle(os.path.join(\"pickle\", \"results.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle(\n",
    "    os.path.join(\"pickle\", \"results.pickle\"))\n",
    "mod_2 = modelrun_text(\n",
    "    text=\"prob_desc\", model=\"2\", variant=\"-\", svd=None, ngram_range=(1, 1))\n",
    "\n",
    "save_results(mod_2, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle(\n",
    "    os.path.join(\"pickle\", \"results.pickle\"))\n",
    "mod_3 = modelrun_text(text=\"prob_desc\", model=\"3\",\n",
    "                   variant=\"ngram_range=(1,2)\", svd=None, ngram_range=(1, 2))\n",
    "save_results(mod_3, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle(\n",
    "    os.path.join(\"pickle\", \"results.pickle\"))\n",
    "mod_4 = modelrun_text(text=\"prob_desc\", model=\"4\",\n",
    "                   variant=\"ngram_range=(1,3)\", svd=None, ngram_range=(1, 3))\n",
    "save_results(mod_4, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle(\n",
    "    os.path.join(\"pickle\", \"results.pickle\"))\n",
    "mod_5 = modelrun_text(text=\"prob_desc_lemmat\", model=\"5\",\n",
    "                   variant=\"lemmatized text\", svd=None, ngram_range=(1, 1))\n",
    "save_results(mod_5, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle(\n",
    "    os.path.join(\"pickle\", \"results.pickle\"))\n",
    "mod_6 = modelrun_text(text=\"prob_desc_no_punc\", model=\"6\",\n",
    "                   variant=\"no punctuation\", svd=None, ngram_range=(1, 1))\n",
    "save_results(mod_6, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle(\n",
    "    os.path.join(\"pickle\", \"results.pickle\"))\n",
    "mod_7 = modelrun_text_tfidf(text=\"prob_desc\", model=\"7\",\n",
    "                         variant=\"TF-IDF\", svd=None, ngram_range=(1, 1))\n",
    "save_results(mod_7, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = pd.read_pickle(\n",
    "    os.path.join(\"pickle\", \"results.pickle\"))\n",
    "mod_8 = modelrun_all_count(text=\"prob_desc\", others=[\n",
    "                                   \"ampdscode\"], model=\"8\", variant=\"-\")\n",
    "save_results(mod_8, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recalculate DeLong CIs but with Bonferroni correction\n",
    "def adjust_ci(x):    \n",
    "    auc, auc_cov = delong_roc_variance(\n",
    "        amb_test[y_varb].values,\n",
    "        x[0][:, 1])\n",
    "    \n",
    "    alpha = 1 - (0.05/len(results))\n",
    "\n",
    "    auc_std = np.sqrt(auc_cov)\n",
    "    lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "    ci = stats.norm.ppf(lower_upper_q, loc=auc, scale=auc_std)\n",
    "    ci[ci > 1] = 1\n",
    "\n",
    "    return f\" {x[1]} ({ci[0].round(3)} - {ci[1].round(3)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column that has both the ROC-AUC point estimate and predicted probabilities\n",
    "results['adjust_roc'] = results.apply(\n",
    "    lambda row: [row[\"Test y_pred_proba\"],row[\"Test roc auc\"]], axis = 1) \n",
    "results[\"Test roc auc (CI)\"] = results['adjust_roc'].apply(lambda x: adjust_ci(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_pickle(\n",
    "    os.path.join(\"pickle\", \"results.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_tables = [\"CV sensitivity\", \"CV specificity\",\n",
    "                   \"CV precision\", \"CV true negative rate\", \"CV roc auc\"]\n",
    "\n",
    "# Shorter names\n",
    "crossval_columns = [\"Sensitivity\", \"Specificity\",\n",
    "                   \"Precision\", \"TN rate\", \"ROC-AUC\"]\n",
    "\n",
    "test_tables = [\"Test sensitivity\", \"Test specificity\", \"Test precision\",\n",
    "               \"Test true negative rate\", \"Test roc auc (CI)\"]\n",
    "\n",
    "# Shorter names\n",
    "test_columns = [\"Sensitivity\", \"Specificity\",\n",
    "                   \"Precision\", \"TN rate\", \"ROC-AUC (CI)\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross validation\n",
    "results_cv = results[crossval_tables]\n",
    "results_cv.columns = crossval_columns\n",
    "results_cv = pd.concat([results_cv], keys=[\"CV\"], names = \"\", axis=1)\n",
    "\n",
    "# Testing\n",
    "results_test = results[test_tables]\n",
    "results_test.columns = test_columns\n",
    "results_test = pd.concat([results_test], keys=[\"Testing\"], names = \"\", axis=1)\n",
    "\n",
    "results_final  = pd.concat([results_cv, results_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Results table for report\n",
    "results_final.to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_final.pickle\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_final.to_csv(\"results_final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities\n",
    "probas_list = [results[\"Test y_pred_proba\"][1][:,1],\n",
    "               results[\"Test y_pred_proba\"][2][:,1],\n",
    "               results[\"Test y_pred_proba\"][4][:,1],\n",
    "               results[\"Test y_pred_proba\"][5][:,1], \n",
    "               results[\"Test y_pred_proba\"][22][:,1],\n",
    "               results[\"Test y_pred_proba\"][23][:,1]]\n",
    "\n",
    "# Model/classifier names\n",
    "clf_names = [results.reset_index()[\"Model\"][1],\n",
    "             results.reset_index()[\"Model\"][2],\n",
    "             results.reset_index()[\"Model\"][4],\n",
    "             results.reset_index()[\"Model\"][5],\n",
    "             results.reset_index()[\"Model\"][22],\n",
    "             results.reset_index()[\"Model\"][23]]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Calibration curve\n",
    "\n",
    "\n",
    "skplt.metrics.plot_calibration_curve(\n",
    "    amb_test[y_varb].values, probas_list, clf_names, ax=ax)\n",
    "fig.savefig(\"figures_new/calib_plot.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taken from:\n",
    "https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Generate a no skill prediction (majority class)\n",
    "sim = [0 for _ in range(len(amb_test))]\n",
    "\n",
    "sim_fpr, sim_tpr, _ = roc_curve(amb_test[y_varb].values, sim)\n",
    "\n",
    "# FPRs and TPRs for each model\n",
    "GBM1_fpr, GBM1_tpr, _ = roc_curve(amb_test[y_varb].values, \n",
    "                                  results[\"Test y_pred_proba\"][1][:,1])\n",
    "RF_bal1_fpr, RF_bal1_tpr, _  = roc_curve(amb_test[y_varb].values, \n",
    "                                         results[\"Test y_pred_proba\"][2][:,1])\n",
    "GBM2_fpr, GBM2_tpr, _  = roc_curve(amb_test[y_varb].values, \n",
    "                                         results[\"Test y_pred_proba\"][3][:,1])\n",
    "\n",
    "RF_bal2_fpr, RF_bal2_tpr, _  = roc_curve(amb_test[y_varb].values, \n",
    "                                         results[\"Test y_pred_proba\"][4][:,1])\n",
    "\n",
    "GBM8_fpr, GBM8_tpr, _  = roc_curve(amb_test[y_varb].values, \n",
    "                                         results[\"Test y_pred_proba\"][22][:,1])\n",
    "\n",
    "RF_bal8_fpr, RF_bal8_tpr, _  = roc_curve(amb_test[y_varb].values, \n",
    "                                         results[\"Test y_pred_proba\"][23][:,1])\n",
    "\n",
    "# Plot the roc curve for each model\n",
    "plt.plot(GBM1_fpr, GBM1_tpr, label='1GBM',color=\"black\", linewidth=2.5)\n",
    "plt.plot(RF_bal1_fpr, RF_bal1_tpr, label='1RF_bal', color=\"blue\", linewidth=2.5)\n",
    "plt.plot(GBM2_fpr, GBM2_tpr, label='2GBM', color=\"deepskyblue\", linewidth=2.5)\n",
    "plt.plot(RF_bal2_fpr, RF_bal2_tpr, label='2RF_bal', color=\"green\", linewidth=2.5)\n",
    "plt.plot(GBM8_fpr, GBM8_tpr, label='8GBM', color=\"greenyellow\", linewidth=2.5)\n",
    "plt.plot(RF_bal8_fpr, RF_bal8_tpr, label='8RF_bal', color=\"red\", linewidth=2.5)\n",
    "plt.plot(sim_fpr, sim_tpr, linestyle='--', label='', color=\"black\", linewidth=2.5)\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "\n",
    "plt.legend(loc=\"lower right\", markerscale=200)\n",
    "\n",
    "plt.savefig(\"figures_new/roc_auc.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10))\n",
    "\n",
    "ax1 = fig.add_subplot(221)\n",
    "\n",
    "plt.subplots_adjust(wspace=0.5, hspace=0.5)\n",
    "\n",
    "# Get features from results tables and sort in descending order or importance\n",
    "pd.Series(results[\"Feature Imps\"][4]).sort_values(\n",
    "    ascending=True).plot.barh(color=\"green\",ax=ax1)\n",
    "ax1.set_xlabel(\"Importance\")\n",
    "ax1.set_ylabel(\"Feature\")\n",
    "ax1.title.set_text('Model 2GBM')\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(222, xmargin=3)\n",
    "pd.Series(results[\"Feature Imps\"][5]).sort_values(\n",
    "    ascending=True).plot.barh(color=\"green\",ax=ax2)\n",
    "ax2.set_xlabel(\"Importance\")\n",
    "ax2.set_ylabel(\"Feature\")\n",
    "ax2.title.set_text('Model 2RF_bal')\n",
    "\n",
    "ax1 = fig.add_subplot(223, ymargin=3)\n",
    "# Get features from results tables and sort in descending order or importance\n",
    "pd.Series(results[\"Feature Imps\"][22]).sort_values(\n",
    "    ascending=True)[10:].plot.barh(color=\"green\",ax=ax1)\n",
    "ax1.set_xlabel(\"Importance\")\n",
    "ax1.set_ylabel(\"Feature\")\n",
    "ax1.title.set_text('Model 8GBM')\n",
    "\n",
    "\n",
    "ax2 = fig.add_subplot(224)\n",
    "pd.Series(results[\"Feature Imps\"][23]).sort_values(\n",
    "    ascending=True)[10:].plot.barh(color=\"green\",ax=ax2)\n",
    "ax2.set_xlabel(\"Importance\")\n",
    "ax2.set_ylabel(\"Feature\")\n",
    "ax2.title.set_text('Model 8RF_bal')\n",
    "fig.savefig(\"figures_new/imp_text.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conveyance rate and occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_made =[\"(ALL INCS)\"]\n",
    "\n",
    "# Chart showing frequency and proportion conveyed by term (\"Skinny fries\" chart\n",
    "sns.set_style(\"dark\")\n",
    "amb_train[\"(ALL INCS)\"] = 1\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "vabs = [\"heart\",\"hx\", \"head\", \"vomiting\", \"dob\",\"and\", \"not\", \"in\", \"(ALL INCS)\"\n",
    "        , \"pat\", \"?\", \"uncons\", \"caller\", \"lying\", \"nfda\", \"homeless\"]\n",
    "\n",
    "# Fix spacing\n",
    "ind = np.arange(len(vabs))*0.5\n",
    "#ind[6:] = ind[6:]+0.5\n",
    "ind[7:] = ind[7:]+0.5\n",
    "ind[8:] = ind[8:]+0.5\n",
    "ind[9:] = ind[9:]+0.5\n",
    "\n",
    "conv = [] # conveyance rate\n",
    "width = [] # bar width\n",
    "\n",
    "# Iterate through all of the terms to get conveyance rate and frequency of occurrence\n",
    "for i in range(len(vabs)):\n",
    "    if vabs[i] not in pre_made:\n",
    "        my_regex = r\"(\" + re.escape(vabs[i]) + r\")\"\n",
    "        amb_train[vabs[i]] = amb_train.prob_desc.apply(\n",
    "            lambda x: 1 if re.search(my_regex, x) else 0)\n",
    "\n",
    "    conv.append(len(amb_train.loc[(amb_train[vabs[i]] == 1) & (\n",
    "        amb_train.conveyed == 1)])*100/len(amb_train[amb_train[vabs[i]] == 1]))\n",
    "\n",
    "    width.append(amb_train[vabs[i]].sum()/len(amb_train))\n",
    "\n",
    "fig = plt.figure(figsize=(13, 7))\n",
    "p1 = plt.bar(ind, conv, width=width, edgecolor=\"green\", color=\"green\")\n",
    "plt.xticks(ind, vabs)\n",
    "plt.ylabel(\"% conveyed\")\n",
    "plt.xlabel(\"Term (bar width proportional to frequency)\")\n",
    "\n",
    "plt.savefig(\"figures_new/skinny.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare string lengths with \"nfda\" in problem description\n",
    "amb_train[amb_train.nfda ==1][\"string_len\"].mean()\n",
    "amb_train[\"string_len\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary stats for train and  test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for displaying cross validation and test set results side by side\n",
    "def desc_stats2(vab):    \n",
    "    a = desc_stats1(vab, amb_train, \"Training set\")\n",
    "    b = desc_stats1(vab, amb_test, \"Test set\")\n",
    "      \n",
    "    z  = pd.concat([a, b], axis=1)\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPDS summary table for report\n",
    "mpds_table = desc_stats2(\"ampds_full\")\n",
    "\n",
    "# Adding an \"all incidents\" row\n",
    "amb_train[\"dummy\"] = 1\n",
    "amb_test[\"dummy\"] = 1\n",
    "all_incs = desc_stats2(\"dummy\")\n",
    "all_incs = all_incs.rename(index={1: 'All incidents'})\n",
    "full_tab = all_incs.append(mpds_table)\n",
    "full_tab.to_csv(\"desc_stats.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text summary stats (as earlier but for test set too)\n",
    "test_text = {\"Average string length\": \n",
    "                amb_test[\"string_len\"].mean().round(1),          \n",
    "                \"Average string length (not conveyed)\":\n",
    "                amb_test.loc[amb_test.conveyed ==0 ][\"string_len\"].mean().round(1),\n",
    "           \"ave_size_unique\": av_length(amb_test)}\n",
    "\n",
    "\n",
    "pd.DataFrame({\"Training set\":training_text,\"Test set\":test_text})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package version numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = [\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"nltk\",\n",
    "            \"spacy\", \"sklearn\", \"wordcloud\", \"scipy\", \"scikitplot\"]\n",
    "\n",
    "versions = [np.__version__, pd.__version__, matplotlib.__version__,\n",
    "            sns.__version__, nltk.__version__, spacy.__version__,\n",
    "            sklearn.__version__, \"1.5.0\", scipy.__version__, skplt.__version__]\n",
    "\n",
    "\n",
    "print(pd.DataFrame.from_dict({\"Package\": packages,\n",
    "                        \"Version\": versions}\n",
    "                      ).set_index(\"Package\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "          Version\n",
    "Package           \n",
    "numpy       1.15.4\n",
    "pandas      0.23.4\n",
    "matplotlib   3.0.2\n",
    "seaborn      0.9.0\n",
    "nltk         3.4.1\n",
    "spacy        2.1.3\n",
    "sklearn     0.20.2\n",
    "wordcloud    1.5.0\n",
    "scipy        1.1.0\n",
    "scikitplot   0.3.7"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "684px",
    "width": "383px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "1094px",
    "left": "10px",
    "top": "150px",
    "width": "320px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "677.847px",
    "left": "1606px",
    "right": "20px",
    "top": "164px",
    "width": "277.969px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
