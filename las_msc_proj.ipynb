{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hide_input": false,
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Preparation\" data-toc-modified-id=\"Preparation-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Preparation</a></span><ul class=\"toc-item\"><li><span><a href=\"#Main-packages-and-config\" data-toc-modified-id=\"Main-packages-and-config-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Main packages and config</a></span></li><li><span><a href=\"#Read-in-data\" data-toc-modified-id=\"Read-in-data-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Read in data</a></span></li><li><span><a href=\"#Setting-outcome-measures\" data-toc-modified-id=\"Setting-outcome-measures-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Setting outcome measures</a></span></li><li><span><a href=\"#Derive-additional-attributes\" data-toc-modified-id=\"Derive-additional-attributes-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Derive additional attributes</a></span></li><li><span><a href=\"#Process-text\" data-toc-modified-id=\"Process-text-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Process text</a></span></li><li><span><a href=\"#Rake-(no-corpus)\" data-toc-modified-id=\"Rake-(no-corpus)-1.6\"><span class=\"toc-item-num\">1.6&nbsp;&nbsp;</span>Rake (no corpus)</a></span></li><li><span><a href=\"#Geotagging\" data-toc-modified-id=\"Geotagging-1.7\"><span class=\"toc-item-num\">1.7&nbsp;&nbsp;</span>Geotagging</a></span></li><li><span><a href=\"#Pickle-with-processed-data\" data-toc-modified-id=\"Pickle-with-processed-data-1.8\"><span class=\"toc-item-num\">1.8&nbsp;&nbsp;</span>Pickle with processed data</a></span></li><li><span><a href=\"#Train---test-split\" data-toc-modified-id=\"Train---test-split-1.9\"><span class=\"toc-item-num\">1.9&nbsp;&nbsp;</span>Train - test split</a></span></li></ul></li><li><span><a href=\"#Exploratory-analysis-(before-dropping-no-dispatch)\" data-toc-modified-id=\"Exploratory-analysis-(before-dropping-no-dispatch)-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Exploratory analysis (before dropping <em>no dispatch</em>)</a></span></li><li><span><a href=\"#Exploratory-analysis-(after-dropping-no-dispatch)\" data-toc-modified-id=\"Exploratory-analysis-(after-dropping-no-dispatch)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Exploratory analysis (after dropping <em>no dispatch</em>)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Call-priority-category\" data-toc-modified-id=\"Call-priority-category-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Call priority category</a></span></li><li><span><a href=\"#MPDS-code\" data-toc-modified-id=\"MPDS-code-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>MPDS code</a></span></li><li><span><a href=\"#Age\" data-toc-modified-id=\"Age-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Age</a></span></li><li><span><a href=\"#Hour\" data-toc-modified-id=\"Hour-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Hour</a></span></li><li><span><a href=\"#Free-text\" data-toc-modified-id=\"Free-text-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Free text</a></span></li><li><span><a href=\"#Geospatial:-coords\" data-toc-modified-id=\"Geospatial:-coords-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Geospatial: coords</a></span></li><li><span><a href=\"#Geospatial:-MSOA\" data-toc-modified-id=\"Geospatial:-MSOA-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Geospatial: MSOA</a></span></li><li><span><a href=\"#Geospatial:-Borough\" data-toc-modified-id=\"Geospatial:-Borough-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Geospatial: Borough</a></span></li></ul></li><li><span><a href=\"#Modelling\" data-toc-modified-id=\"Modelling-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Modelling</a></span><ul class=\"toc-item\"><li><span><a href=\"#Input-data\" data-toc-modified-id=\"Input-data-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Input data</a></span></li><li><span><a href=\"#Model-classifers-and-functions\" data-toc-modified-id=\"Model-classifers-and-functions-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Model classifers and functions</a></span></li><li><span><a href=\"#Model-run-functions\" data-toc-modified-id=\"Model-run-functions-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>Model run functions</a></span></li></ul></li><li><span><a href=\"#Model-runs\" data-toc-modified-id=\"Model-runs-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model runs</a></span><ul class=\"toc-item\"><li><span><a href=\"#Baseline\" data-toc-modified-id=\"Baseline-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Baseline</a></span></li><li><span><a href=\"#A-(MPDS)\" data-toc-modified-id=\"A-(MPDS)-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>A (MPDS)</a></span></li><li><span><a href=\"#B-(free-text)\" data-toc-modified-id=\"B-(free-text)-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>B (free text)</a></span></li><li><span><a href=\"#C-(locations)\" data-toc-modified-id=\"C-(locations)-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>C (locations)</a></span></li><li><span><a href=\"#D-(Numerical)\" data-toc-modified-id=\"D-(Numerical)-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>D (Numerical)</a></span></li><li><span><a href=\"#E-(Mixed)\" data-toc-modified-id=\"E-(Mixed)-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>E (Mixed)</a></span></li><li><span><a href=\"#Extra-text-runs\" data-toc-modified-id=\"Extra-text-runs-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Extra text runs</a></span></li></ul></li><li><span><a href=\"#Results-(RQ-1-2)\" data-toc-modified-id=\"Results-(RQ-1-2)-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Results (RQ 1-2)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Tables\" data-toc-modified-id=\"Tables-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Tables</a></span></li><li><span><a href=\"#Calibration-plot\" data-toc-modified-id=\"Calibration-plot-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Calibration plot</a></span></li></ul></li><li><span><a href=\"#Characteristics-of--(RQ3)\" data-toc-modified-id=\"Characteristics-of--(RQ3)-7\"><span class=\"toc-item-num\">7&nbsp;&nbsp;</span>Characteristics of  (RQ3)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Random-Forest-tree-viz\" data-toc-modified-id=\"Random-Forest-tree-viz-7.1\"><span class=\"toc-item-num\">7.1&nbsp;&nbsp;</span>Random Forest tree viz</a></span></li><li><span><a href=\"#Feature-importances\" data-toc-modified-id=\"Feature-importances-7.2\"><span class=\"toc-item-num\">7.2&nbsp;&nbsp;</span>Feature importances</a></span></li><li><span><a href=\"#Clusters-for-models-C3,-E5\" data-toc-modified-id=\"Clusters-for-models-C3,-E5-7.3\"><span class=\"toc-item-num\">7.3&nbsp;&nbsp;</span>Clusters for models C3, E5</a></span></li><li><span><a href=\"#Conveyance-rate-and-occurance\" data-toc-modified-id=\"Conveyance-rate-and-occurance-7.4\"><span class=\"toc-item-num\">7.4&nbsp;&nbsp;</span>Conveyance rate and occurance</a></span></li><li><span><a href=\"#Call-category-distribution-and-terms\" data-toc-modified-id=\"Call-category-distribution-and-terms-7.5\"><span class=\"toc-item-num\">7.5&nbsp;&nbsp;</span>Call category distribution and terms</a></span></li><li><span><a href=\"#Correctly-classified-as-not-conveyed\" data-toc-modified-id=\"Correctly-classified-as-not-conveyed-7.6\"><span class=\"toc-item-num\">7.6&nbsp;&nbsp;</span>Correctly classified as not conveyed</a></span></li><li><span><a href=\"#Concordance\" data-toc-modified-id=\"Concordance-7.7\"><span class=\"toc-item-num\">7.7&nbsp;&nbsp;</span>Concordance</a></span></li></ul></li><li><span><a href=\"#Package-version-numbers\" data-toc-modified-id=\"Package-version-numbers-8\"><span class=\"toc-item-num\">8&nbsp;&nbsp;</span>Package version numbers</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7CUSMUIP MSc Project: LAS call outcome prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Student number: 1887975\n",
    "\n",
    "I verify that I am the sole author of the programmes contained in this archive, except where explicitly stated to the contrary.\n",
    "\n",
    "Liam Tollinton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main packages and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from datetime import datetime\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 500)\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "\n",
    "# â†“Needed otherwise text string has truncation\n",
    "pd.set_option(\"display.max_colwidth\", 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue = (0.21568627450980393, 0.4941176470588236, 0.7215686274509804)\n",
    "green = (0.3019607843137256, 0.6862745098039216, 0.29019607843137263)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data set\n",
    "las18_p31 = pd.read_csv(\n",
    "    \"~/msc-las/data/nlp_data_extract_010118-311218.csv\", encoding=\"ISO-8859â€“1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting outcome measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Fill nan values in vehicle dispatched column\n",
    "las18_p31[\"vehiclesarrivedx\"] = las18_p31[\"vehiclesarrived\"].fillna(0)\n",
    "\n",
    "# Create binary dispatch attribute\n",
    "las18_p31[\"dispatched\"] = np.where(las18_p31[\"vehiclesarrivedx\"] == 0, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create outcome attribute covering four patient outcome scenarios\n",
    "las18_p31[\"conveyed\"] = las18_p31[\"conveyed\"].fillna(0)\n",
    "las18_p31[\"conveyed\"] = las18_p31[\"conveyed\"].fillna(0)\n",
    "las18_p31[\"not_conveyed\"] = las18_p31.conveyed.replace({0: 1, 1: 0})\n",
    "las18_p31[\"conveyed_ed\"] = las18_p31[\"conveyed_ed\"].fillna(0)\n",
    "las18_p31[\"conveyed_other\"] = las18_p31[\"conveyed\"] - las18_p31[\"conveyed_ed\"]\n",
    "las18_p31[\"outcome_num\"] = las18_p31[\"dispatched\"] + \\\n",
    "    las18_p31[\"conveyed\"] + las18_p31[\"conveyed_ed\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Derive additional attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove false logs\n",
    "las18_p31[\"ampdscode\"] = las18_p31[\"ampdscode\"].str.replace(\n",
    "    \"3.10E+0\", \"31E\", regex=False)\n",
    "\n",
    "# Truncated ampdscode\n",
    "las18_p31[\"ampdscode_trunc\"] = las18_p31[\"ampdscode\"].apply(lambda x: x[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined ampds_full description/code for ease in charts\n",
    "las18_p31[\"ampds_full\"] = las18_p31[[\"ampdscode\", \"description\"]].apply(\n",
    "    lambda x: \" \".join(x), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tidying ampds_full descriptions consistent with https://wiki.radioreference.com/index.php/Priority_Dispatch_Codes\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31D2 Unconscious or Fainting - Effective Breathing\", \n",
    "    \"31D2 Unconscious - Effective Breathing\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31D3 Unconscious or Fainting - Not Alert\", \"31D3 Not Alert\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31D1 Unconscious  Agonal / Ineffective Breathing\", \n",
    "    \"31D1 Unconscious - Agonal / Ineffective Breathing\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31D4 Unconscious or Fainting - Changing Colour\",\n",
    "    \"31D4 Changing Colour\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31D4 Unconscious or Fainting - Changing Colour\", \n",
    "    \"31D4 Changing Colour\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31E2 Unconscious or Fainting - Ineffective Breathing\", \n",
    "    \"31E2 Ineffective Breathing\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31C1 Unconscious or Fainting - Alert with Abnormal Breathing\", \n",
    "    \"31C1 Alert with Abnormal Breathing\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31C1 Unconscious or Fainting - Alert with Abnormal Breathing\", \n",
    "    \"31C1 Alert with Abnormal Breathing\", regex=True)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31E1 Unconscious or Fainting (near) Echo Override\", \n",
    "    \"31E1 Echo Override\", regex=False)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31D0 Unconscious or Fainting (near) Delta Override\", \n",
    "    \"31D0 Delta Override\", regex=False)\n",
    "\n",
    "las18_p31[\"ampds_full\"] = las18_p31[\"ampds_full\"].str.replace(\n",
    "    \"31C0 Unconscious or (Near) Fainting Charlie Override\", \n",
    "    \"31C0 Charlie Override\", regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Change to date_time format\n",
    "las18_p31[\"callstart\"] = pd.to_datetime(las18_p31[\"callstart\"])\n",
    "las18_p31[\"callconcluded\"] = pd.to_datetime(las18_p31[\"callconcluded\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create call length attribute\n",
    "las18_p31[\"call_LEN\"] = las18_p31[\"callconcluded\"] - las18_p31[\"callstart\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cyclical feature technique take from https://ianlondon.github.io/blog/encoding-cyclical-features-24hour-time/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Week of year\n",
    "las18_p31[\"week\"] = las18_p31.callstart.dt.weekofyear\n",
    "\n",
    "# Capture week cycle\n",
    "las18_p31[\"sin_week\"] = np.sin(2*np.pi*las18_p31.week/52)\n",
    "las18_p31[\"cos_week\"] = np.cos(2*np.pi*las18_p31.week/52)\n",
    "\n",
    "# Hour of day\n",
    "las18_p31[\"hour\"] = las18_p31.callstart.dt.hour\n",
    "\n",
    "# Capture cyclicality\n",
    "las18_p31[\"sin_hour\"] = np.sin(2*np.pi*las18_p31.hour/24)\n",
    "las18_p31[\"cos_hour\"] = np.cos(2*np.pi*las18_p31.hour/24)\n",
    "\n",
    "# Day of week\n",
    "las18_p31[\"day\"] = las18_p31.callstart.dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dowdict = {0: \"Mon\", 1: \"Tue\", 2: \"Wed\", 3: \"Thu\", 4: \"Fri\", 5: \"Sat\", 6:\"Sun\"}\n",
    "#las18_p31[\"day\"] = las18_p31[\"day\"].replace(dowdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weekend/fri,sat,sun indicator variables\n",
    "las18_p31[\"fri_sat_sun\"] = np.where((las18_p31[\"day\"] == 4) | (\n",
    "    las18_p31[\"day\"] == 5) | (las18_p31[\"day\"] == 6), 1, 0)\n",
    "las18_p31[\"weekend\"] = np.where(\n",
    "    (las18_p31[\"day\"] == 5) | (las18_p31[\"day\"] == 6), 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create attribute with month\n",
    "las18_p31[\"month\"] = las18_p31[\"callstart\"].apply(lambda x: x.month)\n",
    "# monthdict = {1: \"Jan\", 2: \"Feb\", 3: \"Mar\", 4: \"Apr\", 5: \"May\",\n",
    "# 6: \"Jun\", 7:\"Jul\", 8:\"Aug\", 9:\"Sep\", 10:\"Oct\", 11:\"Nov\", 12:\"Dec\" }\n",
    "#las18_p31[\"month\"] = las18_p31[\"month\"].replace(monthdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rounded age attribute (scatter plot)\n",
    "las18_p31[\"age_rounded\"] = las18_p31.age.round(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add age bands\n",
    "las18_p31[\"age_band\"] = pd.cut(las18_p31.age, bins=[0,5,10,15,20,25,30,35,40,45,50,55,60,65,70,75,80,85,90,95,100,105,110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# String length of problem description\n",
    "las18_p31[\"string_len\"] = las18_p31.problemdescription.str.len()\n",
    "\n",
    "# Lower cased\n",
    "las18_p31[\"prob_desc\"] = las18_p31.problemdescription.astype(\n",
    "    str).apply(lambda x: x.lower())\n",
    "\n",
    "# Drop problem descrption (with date of birth)\n",
    "las18_p31 = las18_p31.drop(\"problemdescription\", axis=1)\n",
    "\n",
    "# Strip numbers\n",
    "las18_p31[\"prob_desc\"] = las18_p31[\"prob_desc\"].apply(\n",
    "    lambda x: re.sub(r\"[0-9]+\", \"\", str(x)))\n",
    "\n",
    "# New attribute with punctucation removed\n",
    "las18_p31[\"prob_desc_no_punc\"] = las18_p31.prob_desc.apply(\n",
    "    lambda x: x.translate(str.maketrans(\"\", \"\", string.punctuation)))\n",
    "\n",
    "# String length after removing punctuation and numbers\n",
    "las18_p31[\"string_len_p\"] = las18_p31[\"prob_desc_no_punc\"].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "\n",
    "\n",
    "def lemma(text):\n",
    "    doc = nlp(text)\n",
    "    # Extract the lemma for each token and join\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "\n",
    "las18_p31[\"prob_desc_lemmat\"] = las18_p31[\"prob_desc\"].apply(\n",
    "    lambda x: lemma(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rake (no corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rake_nltk\n",
    "from rake_nltk import Metric, Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function using rake nltk\n",
    "\n",
    "# Number of keywords\n",
    "kw = 5\n",
    "\n",
    "def rake_text(text):\n",
    "    r = Rake()\n",
    "    r.extract_keywords_from_text(text)\n",
    "    return r.get_ranked_phrases()[0:kw]\n",
    "\n",
    "\n",
    "# Create attribute with keywords\n",
    "las18_p31[\"rake_key_words\"] = las18_p31.prob_desc_no_punc.apply(rake_text)\n",
    "\n",
    "# Convert back to a string\n",
    "las18_p31[\"rake_key_words\"] = las18_p31.rake_key_words.apply(\n",
    "    lambda L: \" \".join(str(x) for x in L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geotagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Incidents with no geotag - very small number means can be dropped\n",
    "print(str(\"Calls in data set set without geotag: \") +\n",
    "      str(len(las18_p31[las18_p31.lat_incident.isna()])))\n",
    "\n",
    "print(str(\"Calls in data set (exc no dispatch incidents) without geotag: \") +\n",
    "      str(len(las18_p31.loc[(las18_p31.outcome_num != 0) \n",
    "                            & (las18_p31.lat_incident.isna())])))\n",
    "\n",
    "las18_p31_with_geo = copy.deepcopy(las18_p31).dropna(subset=['lat_incident'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely\n",
    "from shapely.geometry import Point\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use lat lon to create geotags\n",
    "las18_p31_with_geo.lon_incident = pd.to_numeric(las18_p31_with_geo.lon_incident,\n",
    "                                                errors=\"coerce\")\n",
    "las18_p31_with_geo.lat_incident = pd.to_numeric(las18_p31_with_geo.lat_incident,\n",
    "                                                errors=\"coerce\")\n",
    "las18_p31_with_geo[\"geometry\"] = las18_p31_with_geo.apply(\n",
    "    lambda z: Point(z.lon_incident, z.lat_incident), axis=1)\n",
    "las18_p31_with_geo[\"coords_text\"] = las18_p31_with_geo.lon_incident.map(\n",
    "    str) + \" \" + las18_p31_with_geo.lat_incident.map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MSOAs\n",
    "msoa = gpd.read_file(\n",
    "    \"msoa_shp/Middle_Layer_Super_Output_Areas_December_2011_Boundaries_BSC.shp\",\n",
    "                     crs={\"init\": \"epsg:27700\"})\n",
    "msoa = msoa.to_crs(epsg=4326)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new DataFrame with geometry field only\n",
    "\n",
    "las18_p31_geom = copy.deepcopy(las18_p31_with_geo)[\"geometry\"]\n",
    "\n",
    "# Create GeoDataFrame\n",
    "las18_p31_geom = gpd.GeoDataFrame(\n",
    "    las18_p31_geom, geometry=\"geometry\", crs={\"init\": \"epsg:4326\"})\n",
    "\n",
    "# Add MSOA using a spatial join\n",
    "las18_p31_in_msoa = gpd.sjoin(las18_p31_geom, msoa, op=\"within\")\n",
    "\n",
    "# Merge to add MSOA to main DataFrame\n",
    "las18_p31_with_geo = las18_p31_with_geo.merge(\n",
    "    las18_p31_in_msoa[[\"msoa11nm\"]], how=\"left\",\n",
    "    left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "las18_p31_with_geo = gpd.GeoDataFrame(las18_p31_with_geo, geometry=\"geometry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot incidents which didn't feature an MSOA\n",
    "las18_p31_with_geo.loc[(las18_p31_with_geo.msoa11nm.isna())].plot()\n",
    "\n",
    "# Calls at (50, -7) possibly in Scily Isles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of incidents in Scily Isles?: \" + str(\n",
    "    len(las18_p31_with_geo.loc[(\n",
    "    las18_p31_with_geo.msoa11nm.isna()) & (\n",
    "        las18_p31_with_geo.lat_incident < 50)])))\n",
    "\n",
    "print(\"Number of incidents with dispatch in Scily Isles?: \" + str(\n",
    "    len(las18_p31_with_geo.loc[(\n",
    "    las18_p31_with_geo.msoa11nm.isna()) & (\n",
    "        las18_p31_with_geo.outcome_num != 0) & (\n",
    "        las18_p31_with_geo.lat_incident < 50)])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Scily Isles? calls\n",
    "las18_p31_ml = copy.deepcopy(las18_p31_with_geo\n",
    "                             .loc[las18_p31_with_geo.lat_incident>50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(amb_train.sin_hour, amb_train.cos_hour, c=amb_train.hour)\n",
    "plt.xlabel(\"sine hour\")\n",
    "plt.ylabel(\"cosine hour\")\n",
    "plt.yticks([-1, -0.5, 0, 0.5, 1])\n",
    "plt.savefig(\"figures/time_trick.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot other incidents with no MOSA\n",
    "las18_p31_ml.loc[las18_p31_ml.msoa11nm.isna()].plot()\n",
    "\n",
    "# Incidents \"lost\" in Thames due to MSOA boundary - trunc coordinate problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lat and lon to msoa data frame\n",
    "msoa[\"centroid\"] = msoa.geometry.centroid\n",
    "msoa[\"lat\"] = msoa[\"centroid\"].apply(lambda p: p.y)\n",
    "msoa[\"lon\"] = msoa[\"centroid\"].apply(lambda p: p.x)\n",
    "msoa[\"lad\"] = msoa[\"msoa11nm\"].apply(lambda x: x[:-4])\n",
    "\n",
    "# Add local authority district/borough\n",
    "msoa[\"lad\"] = msoa[\"lad\"].replace(to_replace = \"Kingston upon Thames\",\n",
    "                                  value=\"Kingston\")\n",
    "msoa[\"lad\"] = msoa[\"lad\"].replace(to_replace = \"Richmond upon Thames\",\n",
    "                                  value=\"Richmond\")\n",
    "\n",
    "# Set index as  MSOA name\n",
    "msoa = msoa.set_index(\"msoa11nm\", drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allocate trunc coordinates in Thames to nearest MSOA using MSOA centroids\n",
    "\n",
    "# Function to look up nearest MSOA centroid\n",
    "def fillmsoa(geom):\n",
    "    \n",
    "    x = geom[0].x\n",
    "    y = geom[0].y\n",
    "    \n",
    "    # First cross ref with borough information\n",
    "    if geom[1] in msoa.lad.unique(): \n",
    "        msoa_lad = copy.deepcopy(msoa.loc[msoa.lad == geom[1]])\n",
    "        \n",
    "    else:\n",
    "        msoa_lad = copy.deepcopy(msoa)    \n",
    "                        \n",
    "    msoa_lad[\"x_diff\"] = msoa_lad[\"lon\"].apply(lambda z: z-x)\n",
    "    msoa_lad[\"y_diff\"] = msoa_lad[\"lat\"].apply(lambda z: z-y)\n",
    "    msoa_lad[\"tot_diff\"] = (msoa_lad[\"y_diff\"]**2) + (msoa_lad[\"x_diff\"]**2)\n",
    "    msoa_lad[\"tot_diff_root\"] = msoa_lad[\"tot_diff\"]**0.5\n",
    "    \n",
    "    return msoa_lad[\"tot_diff_root\"].idxmin(axis=1)\n",
    "\n",
    "# New frame with only calls missing MSOA\n",
    "las18_p31_no_msoa = copy.deepcopy(las18_p31_ml.loc[las18_p31_ml.msoa11nm.isna()])\n",
    "\n",
    "# New column with list containing point and borough\n",
    "las18_p31_no_msoa['point_boro'] = las18_p31_no_msoa.apply(\n",
    "    lambda row: [row.geometry,row.borough], axis = 1) \n",
    "\n",
    "# Drop calls missing MSOA from main dataframe\n",
    "las18_p31_with_msoa = copy.deepcopy(las18_p31_ml.dropna(subset=[\"msoa11nm\"]))\n",
    "\n",
    "# Fill using function defined above\n",
    "las18_p31_no_msoa[\"msoa11nm\"] = las18_p31_no_msoa.point_boro.apply(\n",
    "    lambda k: fillmsoa(k))\n",
    "\n",
    "# Drop the temporary point_boro column\n",
    "las18_p31_no_msoa = las18_p31_no_msoa.drop(\"point_boro\", axis=1)\n",
    "\n",
    "# Fill using function defined above\n",
    "las18_p31_final = las18_p31_with_msoa.append(las18_p31_no_msoa).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check no remaining calls missing MSOA info\n",
    "len(las18_p31_final.loc[las18_p31_final.msoa11nm.isna()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pickle with processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â Create a Pickle with processed data\n",
    "import os\n",
    "\n",
    "#las18_p31_final.to_pickle(os.path.join(\"pickle\",\"las18_p31_finalA.pickle\"))\n",
    "\n",
    "#del(las18_p31)\n",
    "#las18_p31_final = pd.read_pickle(os.path.join(\"pickle\",\n",
    "                                              #\"las18_p31_finalA.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train - test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified sample (to preserve train-test ratio after dropping no-dispatch calls)\n",
    "\n",
    "amb_train_0, amb_test_0 = train_test_split(\n",
    "    las18_p31_final, test_size=0.20, random_state=100, \n",
    "    stratify=las18_p31_final[\"outcome_num\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis (before dropping *no dispatch*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(amb_train_0))\n",
    "print(len(amb_test_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(amb_train_0[amb_train_0.dohcategory.isna()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New feature with outcome name to replace number\n",
    "\n",
    "outcome_dict = {0: \"no dispatch\", 1: \"not conveyed\",\n",
    "                2: \"conveyed to other\", 3: \"conveyed to ED\"}\n",
    "amb_train_0[\"outcome\"] = amb_train_0.outcome_num.replace(outcome_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome summary table\n",
    "outcome_table = pd.DataFrame(amb_train_0.groupby(\n",
    "    by=\"outcome\").count()[\"incidentid\"])\n",
    "outcome_table[\"Per cent\"] = outcome_table.apply(lambda x: x/x.sum())\n",
    "outcome_table = outcome_table.rename(\n",
    "    columns={\"incidentid\": \"Number of calls\"}).reset_index()\n",
    "outcome_table[\"label\"] = outcome_table[\"outcome\"] + \\\n",
    "    \" (\" + (outcome_table[\"Per cent\"]*100).round(0).map(str) + \"%)\"\n",
    "outcome_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import squarify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree map plot\n",
    "squarify.plot(sizes=outcome_table['Per cent'],\n",
    "              label=outcome_table[\"label\"], alpha=.8)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amb_train_0.defaultdohcategory.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amb_train_0[\"dohcategory\"] = amb_train_0[\"dohcategory\"].fillna(\"Not assigned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category outcome summary table\n",
    "cat_outcome = pd.DataFrame(amb_train_0.groupby(\n",
    "    by=[\"dohcategory\", \"outcome\"]).count()[\"incidentid\"]).unstack()\n",
    "cat_outcome.columns = cat_outcome.columns.droplevel()\n",
    "\n",
    "# List for Sankey\n",
    "cat_outcome_ls = cat_outcome[[\"not conveyed\", \"no dispatch\",\n",
    "                              \"conveyed to other\", \n",
    "                              \"conveyed to ED\"]].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for priority category side of Sankey\n",
    "label_cat = cat_outcome.sum(axis=1).reset_index()\n",
    "label_cat.columns = [\"name\", \"valuex\"]\n",
    "label_cat[\"label\"] = label_cat.name + \\\n",
    "    \" (\" + label_cat.valuex.astype(int).map(str) + \")\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels for outcome side of Sankey\n",
    "label_outcome = cat_outcome.sum().reset_index()\n",
    "label_outcome.columns = [\"name\", \"valuex\"]\n",
    "label_outcome[\"label\"] = label_outcome.name + \\\n",
    "    \" (\" + label_outcome.valuex.astype(int).map(str) + \")\"\n",
    "label_outcome = label_outcome.reset_index().sort_values(by=\"index\",\n",
    "                                                        ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# All labels DataFrame\n",
    "sank1_labs = label_cat.append(label_outcome)[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Sankey diagram\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=sank1_labs,\n",
    "    ), textfont=dict(size=12),\n",
    "    link=dict(\n",
    "        # Indices correspond to left labels, eg C2, C1, C3\n",
    "        source=[0]*4 + [1]*4 + [2]*4 + [3]*4 + [4]*4 + [5]*4,\n",
    "        # Indices correspond to right labels, eg conveyed to ED, no dispatch etc\n",
    "        target=[6, 7, 8, 9]*6,\n",
    "        value=[j for i in cat_outcome_ls for j in i]\n",
    "    ))])\n",
    "\n",
    "#fig.update_layout(title_text=\"Call categories and patient outcomes\", font_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sankey diagram as above\n",
    "\n",
    "cat_def_act = pd.DataFrame(amb_train_0.groupby(\n",
    "    by=[\"defaultdohcategory\", \"dohcategory\"]).count()[\"incidentid\"]).unstack()\n",
    "cat_def_act.columns = cat_def_act.columns.droplevel()\n",
    "cat_def_act\n",
    "cat_def = cat_def_act.sum().reset_index()\n",
    "cat_def.columns = [\"name\", \"valuex\"]\n",
    "cat_def[\"label\"] = cat_def.name + \\\n",
    "    \" (\" + cat_def.valuex.astype(int).map(str) + \")\"\n",
    "cat_def = cat_def.reset_index().sort_values(by=\"index\", ascending=True)\n",
    "cat_act = cat_def_act.sum(axis=1).reset_index()\n",
    "cat_act.columns = [\"name\", \"valuex\"]\n",
    "cat_act[\"label\"] = cat_act.name + \\\n",
    "    \" (\" + cat_act.valuex.astype(int).map(str) + \")\"\n",
    "sank2_labels = cat_act.append(cat_def)[\"label\"]\n",
    "cat_def_act_ls = cat_def_act[[\"C1 \", \"C2 \",\n",
    "                              \"C3 \", \"C4 \", \"C5 \",\n",
    "                              \"Not assigned\"]].values.tolist()\n",
    "\n",
    "fig = go.Figure(data=[go.Sankey(\n",
    "    node=dict(\n",
    "        pad=15,\n",
    "        thickness=20,\n",
    "        line=dict(color=\"black\", width=0.5),\n",
    "        label=sank2_labels,\n",
    "    ), textfont=dict(size=12),\n",
    "    link=dict(\n",
    "        # indices correspond to cat_defs, eg A1, A2, A2, B1, ...\n",
    "        source=[0]*6 + [1]*6 + [2]*6 + [3]*6,\n",
    "        target=[4, 5, 6, 7, 8, 9]*4,\n",
    "        value=[j for i in cat_def_act_ls for j in i]\n",
    "    ))])\n",
    "\n",
    "#fig.update_layout(title_text=\"Call categories and patient outcomes\", font_size=10)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory analysis (after dropping *no dispatch*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only calls for which ambulance was dispatched\n",
    "amb_train = amb_train_0.loc[amb_train_0.dispatched != 0]\n",
    "amb_train = copy.deepcopy(amb_train)\n",
    "\n",
    "amb_test = amb_test_0.loc[amb_test_0.dispatched != 0]\n",
    "amb_test = copy.deepcopy(amb_test)\n",
    "\n",
    "no_disp = len(amb_train_0) - len(amb_train)\n",
    "str(\"Number of calls with no dispatch: \") + str(no_disp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amb_train.groupby(\"conveyed\").count()[\n",
    "    \"incidentid\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall conveyed split\n",
    "amb_train.groupby(\"conveyed\").count()[\n",
    "    \"incidentid\"].apply(lambda x: x/len(amb_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate summary table for report\n",
    "\n",
    "def desc_stats(vab):\n",
    "    # Training set\n",
    "    x = amb_train.groupby(by=[vab, \"conveyed\"]).count()[\"incidentid\"].unstack()\n",
    "    x[\"N\"] = x.sum(axis=1).astype(int) # Total (conveyed + not conveyed) \n",
    "    x[\"N (%)\"] = x.N/x.N.sum()*100  # % of total\n",
    "    x[\"N (%)\"] = x[\"N (%)\"].round(1)\n",
    "    x[\"% conveyed\"] = x[1.0]/x[\"N\"]*100 # Conveyance rate\n",
    "    x[\"% conveyed\"] = x[\"% conveyed\"].round(1)\n",
    "    x = x.rename(columns={1.0: \"Conveyed\"})\n",
    "    x.index.name = None\n",
    "    x = x.drop(columns=0.0)\n",
    "    x = x[[\"N\", \"N (%)\", \"Conveyed\", \n",
    "       \"% conveyed\"]].rename_axis(\"\", axis=\"columns\") # Appropriate column order\n",
    "    x = pd.concat([x], keys=[\"Training set\"], names=[\"\"], axis=1)\n",
    "    \n",
    "    # Test set\n",
    "    y = amb_test.groupby(by=[vab, \"conveyed\"]).count()[\"incidentid\"].unstack()\n",
    "    y[\"N\"] = y.sum(axis=1).astype(int)\n",
    "    y[\"N (%)\"] = y.N/y.N.sum()*100\n",
    "    y[\"N (%)\"] = y[\"N (%)\"].round(1)\n",
    "    y[\"% conveyed\"] = y[1.0]/y[\"N\"]*100\n",
    "    y[\"% conveyed\"] = y[\"% conveyed\"].round(1)\n",
    "    y = y.rename(columns={1.0: \"Conveyed\"})\n",
    "    y.index.name = None\n",
    "    y = y.drop(columns=0.0)\n",
    "    y = y[[\"N\", \"N (%)\", \"Conveyed\", \n",
    "       \"% conveyed\"]].rename_axis(\"\", axis=\"columns\")\n",
    "    y = pd.concat([y], keys=[\"Test set\"], names=[\"\"], axis=1)\n",
    "    \n",
    "    z  = pd.concat([x, y], axis=1)\n",
    "    \n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call priority category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of incidents and conveyance by call priority category\n",
    "print(desc_stats(\"dohcategory\").to_latex(index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPDS code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Unique MPDS codes\n",
    "len(amb_train.ampds_full.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPDS summary table for report\n",
    "\n",
    "print(desc_stats(\"ampds_full\").to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Top 5 MPDS codes share of total incidents\n",
    "pd.DataFrame(amb_train.groupby(by=\"ampds_full\").count()[\"incidentid\"].sort_values(\n",
    "    ascending=False)).apply(lambda x: x/x.sum(axis=0)).head(5).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Horzontal stacked bar chart with absolute values\n",
    "\n",
    "def stacked_bar_abs(data, attrib1, attrib2, attrib2a, cat_num, title):\n",
    "    fig = plt.figure(figsize=(7, 5.5))\n",
    "    ax = fig.add_subplot(111)\n",
    "    pd.crosstab(data[attrib1], \n",
    "                data[attrib2],\n",
    "                margins=True).sort_values(\n",
    "        by=\"All\", ascending=False).head(cat_num).sort_values(\n",
    "        by=\"All\").drop(\"All\", axis=1).drop(\n",
    "        \"All\", axis=0).plot.barh(stacked=True, ax=ax)\n",
    "    ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    plt.title(title)\n",
    "    ax.set_xlabel(\"Frequency\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# MPDS code with priority cat (colour)\n",
    "stacked_bar_abs(amb_train, \"ampds_full\", \"dohcategory\", \"C2 \",\n",
    "                30, \"AMPDS code frequency (with priority code)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MPDS code with trunc AMPDS code (colour)\n",
    "stacked_bar_abs(amb_train, \"ampds_full\", \"ampdscode_trunc\", \"C2 \",\n",
    "                30, \"AMPDS code frequency (with trunc AMPDS code)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Conveyance by call category\n",
    "sns.catplot(y=\"ampds_full\", x=\"conveyed\", kind=\"bar\",\n",
    "            data=amb_train, orient=\"h\", color=blue, height=5, aspect=2.2)\n",
    "plt.xlabel(\"Proportion conveyed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age summary table for report\n",
    "print(desc_stats(\"age_band\").to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age plot\n",
    "age_agg = pd.DataFrame(amb_train.groupby(\n",
    "    by=[\"age_band\", \"conveyed\"]).count()[\"incidentid\"]).unstack()\n",
    "age_agg.columns = age_agg.columns.droplevel(0)\n",
    "age_agg[\"Incident volume\"] = age_agg[0.0] + age_agg[1.0]\n",
    "age_agg[\"% conveyed\"] = age_agg[1.0]*100/ age_agg[\"Incident volume\"]\n",
    "\n",
    "bands = pd.IntervalIndex(age_agg.reset_index()[\"age_band\"]).mid\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.set_xlabel('Age')\n",
    "ax1.grid(False)\n",
    "ax1.set_ylabel('Incident volume', color=\"darkgreen\")\n",
    "ax1.bar(bands, age_agg[\"Incident volume\"], color=green, width=4)\n",
    "ax1.tick_params(axis='y', labelcolor=\"darkgreen\")\n",
    "ax1.set_xlim(0,110)\n",
    "ax1.set_xticks(np.arange(0,120,10))\n",
    "ax1.set_ylim(0,6000)\n",
    "ax1.set_yticks(np.arange(0,6500,500))\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "ax2.set_ylabel('% conveyed', color=\"darkblue\")\n",
    "ax2.scatter(bands, age_agg[\"% conveyed\"], color=\"darkblue\")\n",
    "ax2.plot(bands, age_agg[\"% conveyed\"], color=\"darkblue\")\n",
    "ax2.tick_params(axis='y', labelcolor=\"darkblue\")\n",
    "ax2.set_ylim(0,100)\n",
    "ax2.set_yticks(np.arange(0,110,10))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"figures/age.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hour summary table for report\n",
    "print(desc_stats(\"hour\").to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hour plot\n",
    "hour_agg = pd.DataFrame(amb_train.groupby(\n",
    "    by=[\"hour\", \"conveyed\"]).count()[\"incidentid\"]).unstack()\n",
    "hour_agg.columns = hour_agg.columns.droplevel(0)\n",
    "hour_agg[\"Incident volume\"] = hour_agg[0.0] + hour_agg[1.0]\n",
    "hour_agg[\"% conveyed\"] = hour_agg[1.0]*100/ hour_agg[\"Incident volume\"]\n",
    "\n",
    "bands = hour_agg.reset_index()[\"hour\"]-0.5\n",
    "\n",
    "sns.set_style(\"dark\")\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax1 = fig.add_subplot(111)\n",
    "\n",
    "ax1.set_xlabel('Hour')\n",
    "ax1.grid(False)\n",
    "ax1.set_ylabel('Incident volume', color=\"darkgreen\")\n",
    "ax1.bar(bands, hour_agg[\"Incident volume\"], color=green, width=0.9)\n",
    "ax1.tick_params(axis='y', labelcolor=\"darkgreen\")\n",
    "ax1.set_xlim(-1,23)\n",
    "ax1.set_xticks(np.arange(1,24,2))\n",
    "ax1.set_xticklabels(np.arange(2,24,2))\n",
    "ax1.set_ylim(0,4500)\n",
    "ax1.set_yticks(np.arange(0,5000,500))\n",
    "\n",
    "ax2 = ax1.twinx()  # Create second axes that shares the same x axis\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('% conveyed', color=\"darkblue\")\n",
    "ax2.scatter(bands, hour_agg[\"% conveyed\"], color=\"darkblue\")\n",
    "ax2.plot(bands, hour_agg[\"% conveyed\"], color=\"darkblue\")\n",
    "ax2.tick_params(axis='y', labelcolor=\"darkblue\")\n",
    "ax2.set_ylim(0,100)\n",
    "ax2.set_ylim(0,100)\n",
    "ax2.set_yticks(np.arange(0,110,10))\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "fig.savefig(\"figures/hour.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Free text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free text summary stats\n",
    "print(str(\"Average raw string length training set: \") + str(\n",
    "    amb_train[\"string_len\"].mean()))\n",
    "print(str(\"Average raw string length test set: \") + str(\n",
    "    amb_test[\"string_len\"].mean()))\n",
    "\n",
    "print(str(\"Average raw string length training set (conveyed): \") + str(\n",
    "    amb_train.loc[amb_train.conveyed ==0 ][\"string_len\"].mean()))\n",
    "\n",
    "print(str(\"Average raw string length test set (conveyed): \") + str(\n",
    "    amb_test.loc[amb_test.conveyed ==0 ][\"string_len\"].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# !!! SLOW RUN TIME \n",
    "\n",
    "# Average token length (excluding punctuation) TRAINING SET\n",
    "free_text = amb_train.prob_desc_no_punc.to_string(index=False)\n",
    "free_text = nltk.word_tokenize(free_text)\n",
    "\n",
    "all_lengths = []\n",
    "num_of_strings = len(free_text)\n",
    "\n",
    "for item in free_text:\n",
    "    if item not in string.punctuation:\n",
    "        string_size = len(item)\n",
    "        all_lengths.append(string_size)\n",
    "        total_size = sum(all_lengths)\n",
    "ave_size = float(total_size) / float(num_of_strings)\n",
    "ave_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average token length (excluding punctuation) TEST SET\n",
    "free_text = amb_test.prob_desc_no_punc.to_string(index=False)\n",
    "free_text = nltk.word_tokenize(free_text)\n",
    "\n",
    "all_lengths = []\n",
    "num_of_strings = len(free_text)\n",
    "\n",
    "for item in free_text:\n",
    "    if item not in string.punctuation:\n",
    "        string_size = len(item)\n",
    "        all_lengths.append(string_size)\n",
    "        total_size = sum(all_lengths)\n",
    "ave_size = float(total_size) / float(num_of_strings)\n",
    "ave_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud\n",
    "from wordcloud import WordCloud, ImageColorGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All text as one string\n",
    "all_text = amb_train.prob_desc.to_string(index=False)\n",
    "\n",
    "# Make Word Cloud\n",
    "wordcloud = WordCloud(background_color=\"white\",\n",
    "                      width=1009, height=750).generate(all_text)\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "wordcloud.to_file(\"figures/cloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Unigram, bigram and trigram plots excluding punctuation\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "fig.subplots_adjust(wspace=1)\n",
    "\n",
    "free_text = amb_train.prob_desc_no_punc.to_string(index=False)\n",
    "free_text = nltk.word_tokenize(free_text)\n",
    "\n",
    "unigrams = nltk.FreqDist(free_text).most_common(20) # Only top 20\n",
    "unigrams = list(zip(*unigrams)) # Separate list of pairs in to list of 2 tuples\n",
    "unigrams_bg = list(unigrams[0])\n",
    "unigrams_bg.reverse() # Needed to get top bar as highest \n",
    "unigrams_freq = list(unigrams[1])\n",
    "unigrams_freq.reverse() # Needed to get top bar as highest\n",
    "ax1 = fig.add_subplot(1, 3, 1) # Left plot\n",
    "ax1.barh(unigrams_bg, unigrams_freq, color=green)\n",
    "ax1.set_title(\"Unigrams\")\n",
    "\n",
    "bigrams = nltk.FreqDist(nltk.bigrams(free_text)).most_common(20)\n",
    "bigrams_bg = [\" \".join(pair[0]) for pair in bigrams]\n",
    "bigrams_bg.reverse()\n",
    "bigrams_freq = list(list(zip(*bigrams))[1])\n",
    "bigrams_freq.reverse()\n",
    "ax2 = fig.add_subplot(1, 3, 2)\n",
    "ax2.barh(bigrams_bg, bigrams_freq, color=green)\n",
    "ax2.set_title(\"Bigrams\")\n",
    "\n",
    "trigrams = nltk.FreqDist(nltk.trigrams(free_text)).most_common(25)\n",
    "trigrams_bg = [\" \".join(pair[0]) for pair in trigrams]\n",
    "trigrams_bg.reverse()\n",
    "trigrams_freq = list(list(zip(*trigrams))[1])\n",
    "trigrams_freq.reverse()\n",
    "ax3 = fig.add_subplot(1, 3, 3)\n",
    "ax3.barh(trigrams_bg, trigrams_freq, color=green)\n",
    "ax3.set_title(\"Trigrams\")\n",
    "fig.savefig(\"figures/ngrams.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial: coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(\"Number of unique locations in data set: \") + \\\n",
    "    str(len(amb_train.coords_text.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "from shapely import geometry\n",
    "\n",
    "# Truncated coordinate aggregate incident count and conveyance rate\n",
    "\n",
    "trunc_agg = amb_train.groupby(\n",
    "    by=[\"lat_incident\", \"lon_incident\", # All incident count\n",
    "        \"coords_text\"])[\"incidentid\"].count().reset_index().merge(\n",
    "    amb_train.groupby(by=[\"lat_incident\", \"lon_incident\", # Conveyed count\n",
    "                          \"coords_text\"])[\"conveyed\"].sum().reset_index(),\n",
    "    how=\"left\").rename(\n",
    "    columns={\"incidentid\": \"Incident volume\",\n",
    "             \"conveyed\": \"conveyed_tot\"}).merge(amb_train.groupby(\n",
    "    by=[\"lat_incident\", \"lon_incident\",\n",
    "        \"coords_text\"])[\"conveyed\"].mean().reset_index(), how=\"left\").rename(\n",
    "    columns={\"conveyed\": \"Proportion conveyed\"}) # Conveyance rate\n",
    "\n",
    "# Change proportion to %\n",
    "trunc_agg[\"% conveyed\"] = trunc_agg[\"Proportion conveyed\"]*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volume of incidents by neighbourhood distribution\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "dist = sns.distplot(trunc_agg[\"Incident volume\"], bins=40)\n",
    "plt.title(\"Distribution of incident volumes by neighbourhood\")\n",
    "dist.set(xlabel=\"Number of incidents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Points correspond to unique locations\n",
    "sns.set()\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "pd.DataFrame(trunc_agg).plot.scatter(\n",
    "    \"Incident volume\", \"% conveyed\", ax=ax, color=green)\n",
    "fig.savefig(\"figures/loc.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in Borough boundary data\n",
    "boroughs = gpd.read_file(\"Borough/boroughs.shp\", crs={\"init\": \"epsg:27700\"})\n",
    "boroughs = boroughs.to_crs(epsg=4326)\n",
    "\n",
    "boroughs[\"dis\"] = np.zeros\n",
    "\n",
    "# Disolve boundaries\n",
    "london = gpd.GeoDataFrame(boroughs.dissolve(by=\"dis\")[\"geometry\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voronoi function taken from https://gist.github.com/pv/8036995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libpysal.cg.voronoi import Voronoi, voronoi_frames\n",
    "from shapely.geometry import Polygon\n",
    "\n",
    "\n",
    "def voronoi_finite_polygons_2d(vor, radius=None):\n",
    "    \"\"\"\n",
    "    Reconstruct infinite voronoi regions in a 2D diagram to finite\n",
    "    regions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    vor : Voronoi\n",
    "        Input diagram\n",
    "    radius : float, optional\n",
    "        Distance to \"points at infinity\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    regions : list of tuples\n",
    "        Indices of vertices in each revised Voronoi regions.\n",
    "    vertices : list of tuples\n",
    "        Coordinates for revised Voronoi vertices. Same as coordinates\n",
    "        of input vertices, with \"points at infinity\" appended to the\n",
    "        end.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if vor.points.shape[1] != 2:\n",
    "        raise ValueError(\"Requires 2D input\")\n",
    "\n",
    "    new_regions = []\n",
    "    new_vertices = vor.vertices.tolist()\n",
    "\n",
    "    center = vor.points.mean(axis=0)\n",
    "    if radius is None:\n",
    "        radius = vor.points.ptp().max()\n",
    "\n",
    "    # Construct a map containing all ridges for a given point\n",
    "    all_ridges = {}\n",
    "    for (p1, p2), (v1, v2) in zip(vor.ridge_points, vor.ridge_vertices):\n",
    "        all_ridges.setdefault(p1, []).append((p2, v1, v2))\n",
    "        all_ridges.setdefault(p2, []).append((p1, v1, v2))\n",
    "\n",
    "    # Reconstruct infinite regions\n",
    "    for p1, region in enumerate(vor.point_region):\n",
    "        vertices = vor.regions[region]\n",
    "\n",
    "        if all(v >= 0 for v in vertices):\n",
    "            # finite region\n",
    "            new_regions.append(vertices)\n",
    "            continue\n",
    "\n",
    "        # reconstruct a non-finite region\n",
    "        ridges = all_ridges[p1]\n",
    "        new_region = [v for v in vertices if v >= 0]\n",
    "\n",
    "        for p2, v1, v2 in ridges:\n",
    "            if v2 < 0:\n",
    "                v1, v2 = v2, v1\n",
    "            if v1 >= 0:\n",
    "                # finite ridge: already in the region\n",
    "                continue\n",
    "\n",
    "            # Compute the missing endpoint of an infinite ridge\n",
    "\n",
    "            t = vor.points[p2] - vor.points[p1]  # tangent\n",
    "            t /= np.linalg.norm(t)\n",
    "            n = np.array([-t[1], t[0]])  # normal\n",
    "\n",
    "            midpoint = vor.points[[p1, p2]].mean(axis=0)\n",
    "            direction = np.sign(np.dot(midpoint - center, n)) * n\n",
    "            far_point = vor.vertices[v2] + direction * radius\n",
    "\n",
    "            new_region.append(len(new_vertices))\n",
    "            new_vertices.append(far_point.tolist())\n",
    "\n",
    "        # sort region counterclockwise\n",
    "        vs = np.asarray([new_vertices[v] for v in new_region])\n",
    "        c = vs.mean(axis=0)\n",
    "        angles = np.arctan2(vs[:, 1] - c[1], vs[:, 0] - c[0])\n",
    "        new_region = np.array(new_region)[np.argsort(angles)]\n",
    "\n",
    "        # finish\n",
    "        new_regions.append(new_region.tolist())\n",
    "\n",
    "    return new_regions, np.asarray(new_vertices)\n",
    "\n",
    "\n",
    "# Create normal voronoi\n",
    "vor = Voronoi(np.c_[trunc_agg.lon_incident, trunc_agg.lat_incident])\n",
    "\n",
    "# Finite region voronoi\n",
    "regions, vertices = voronoi_finite_polygons_2d(vor)\n",
    "poly = Polygon(vertices)\n",
    "\n",
    "# Create list of vertices in each polygon\n",
    "cells = [vertices[region] for region in regions]\n",
    "listy = []\n",
    "for cell in cells:\n",
    "    polygon = Polygon(cell)\n",
    "    listy.append(polygon)\n",
    "\n",
    "# New column in data frame with geometry\n",
    "trunc_agg[\"geometry\"] = listy\n",
    "trunc_agg = gpd.GeoDataFrame(\n",
    "    trunc_agg, geometry=\"geometry\", crs={\"init\": \"epsg:4326\"})\n",
    "\n",
    "# Use merged London borough boundaries to cut polygons\n",
    "trunc_agg = gpd.overlay(trunc_agg, london, how=\"intersection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from shapely import geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normal voronoi\n",
    "vor = Voronoi(np.c_[trunc_agg.lon_incident, trunc_agg.lat_incident])\n",
    "\n",
    "# Finite region voronoi\n",
    "regions, vertices = voronoi_finite_polygons_2d(vor)\n",
    "poly = geometry.Polygon(vertices)\n",
    "\n",
    "# Create list of vertices in each polygon\n",
    "cells = [vertices[region] for region in regions]\n",
    "listy = []\n",
    "for cell in cells:\n",
    "    polygon = Polygon(cell)\n",
    "    listy.append(polygon)\n",
    "\n",
    "# New column in data frame with geometry\n",
    "trunc_agg[\"geometry\"] = listy\n",
    "trunc_agg = gpd.GeoDataFrame(\n",
    "    trunc_agg, geometry=\"geometry\", crs={\"init\": \"epsg:4326\"})\n",
    "\n",
    "# Use merged London borough boundaries to cut polygons\n",
    "trunc_agg = gpd.overlay(trunc_agg, london, how=\"intersection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique locations in training set\n",
    "trunc_agg.describe().iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextily as ctx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basemap function from https://geopandas.readthedocs.io/en/latest/gallery/plotting_basemap_background.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_basemap(ax, zoom, url='http://tile.stamen.com/terrain/tileZ/tileX/tileY.png'):\n",
    "    xmin, xmax, ymin, ymax = ax.axis()\n",
    "    basemap, extent = ctx.bounds2img(\n",
    "        xmin, ymin, xmax, ymax, zoom=zoom, url=url)\n",
    "    ax.imshow(basemap, extent=extent, interpolation='bilinear')\n",
    "    # restore original x/y limits\n",
    "    ax.axis((xmin, xmax, ymin, ymax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Change crs\n",
    "trunc_agg = trunc_agg.to_crs(epsg=3857)\n",
    "\n",
    "# Unique location map\n",
    "fig, ax = plt.subplots(figsize=(30, 10))\n",
    "\n",
    "# Plot the polygons\n",
    "trunc_agg.plot(column=\"Incident volume\", ax=ax, cmap=\"viridis\", legend=True)\n",
    "\n",
    "add_basemap(ax, zoom=11)\n",
    "\n",
    "# North arrow\n",
    "ax.text(30000, 6680000, '\\u25B2 \\nN ', ha='center', fontsize=40,\n",
    "        family='Arial', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "ax.set_xlabel(\"Easting (WGS84/Pseudo-Mercator)\")\n",
    "ax.set_ylabel(\"Northing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial: MSOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate incidents to MSOA level\n",
    "msoa_agg = pd.DataFrame(amb_train.groupby(\n",
    "    by=[\"msoa11nm\", \"conveyed\"]).count()[\"incidentid\"]).unstack()\n",
    "msoa_agg.columns = msoa_agg.columns.droplevel(0)\n",
    "msoa_agg[\"Incident volume\"] = msoa_agg[0.0] + msoa_agg[1.0]\n",
    "msoa_agg[\"% conveyed\"] = msoa_agg[1.0]*100/ msoa_agg[\"Incident volume\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GeoDataFrame by merging with boundaries\n",
    "msoa_agg = gpd.GeoDataFrame(msoa_agg.merge(\n",
    "    msoa, left_index=True, right_on=\"msoa11nm\"), geometry=\"geometry\")\n",
    "msoa_agg.crs = {'init' :'epsg:4326'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot choropleth map proportion conveyed by MSOA\n",
    "\n",
    "# Change crs\n",
    "msoa_agg = msoa_agg.to_crs(epsg=3857)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 10.5))\n",
    "\n",
    "# Plot the polygons\n",
    "msoa_agg.dropna().plot(column=\"% conveyed\",\n",
    "                       legend=True, ax=ax, cmap=\"viridis\")\n",
    "\n",
    "add_basemap(ax, zoom=11)\n",
    "\n",
    "# North arrow\n",
    "ax.text(30000, 6680000, '\\u25B2 \\nN ', ha='center', fontsize=40,\n",
    "        family='Arial', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "ax.set_xlabel(\"Easting (WGS84/Pseudo-Mercator)\")\n",
    "ax.set_ylabel(\"Northing\")\n",
    "fig.savefig(\"figures/msoa_map.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geospatial: Borough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Brough summary table for report\n",
    "print(desc_stats(\"borough\").sort_values(by=(\"Training set\", \"% conveyed\")).to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot proportion conveyed by MSOA\n",
    "\n",
    "# Convert CRS of borough bounary file\n",
    "from adjustText import adjust_text\n",
    "boroughs = boroughs.to_crs(epsg=3857)\n",
    "\n",
    "borough_agg = pd.DataFrame(amb_train.groupby(\n",
    "    by=[\"borough\", \"conveyed\"]).count()[\"incidentid\"]).unstack()\n",
    "\n",
    "# Reconcile difference spellings\n",
    "borough_agg = borough_agg.reset_index().replace(\n",
    "    to_replace=\"Richmond\", value=\"Richmond upon Thames\")\n",
    "borough_agg = borough_agg.reset_index().replace(\n",
    "    to_replace=\"Kingston\", value=\"Kingston upon Thames\")\n",
    "borough_agg = borough_agg.set_index(\"borough\")\n",
    "\n",
    "borough_agg.columns = borough_agg.columns.droplevel(0)\n",
    "borough_agg = borough_agg.apply(lambda x: x*100/borough_agg.sum(axis=1))\n",
    "borough_agg = boroughs[[\"geometry\", \"NAME\"]].merge(\n",
    "    borough_agg, left_on=\"NAME\", right_index=True, how=\"left\")\n",
    "borough_agg[\"% conveyed\"] = np.where(\n",
    "    borough_agg[1.0] != 1, borough_agg[1.0], np.nan)\n",
    "\n",
    "# Make figure\n",
    "fig, ax = plt.subplots(figsize=(15, 10.5))\n",
    "\n",
    "# Plot the polygons\n",
    "borough_agg.dropna().plot(column=\"% conveyed\",\n",
    "                          legend=True, ax=ax, cmap=\"viridis\", alpha=0.8)\n",
    "\n",
    "\n",
    "# North arrow\n",
    "ax.text(30000, 6680000, '\\u25B2 \\nN ', ha='center', fontsize=40,\n",
    "        family='Arial', bbox=dict(facecolor='white', alpha=0.5))\n",
    "\n",
    "# Borough labels\n",
    "borough_agg['coords'] = borough_agg['geometry'].apply(\n",
    "    lambda x: x.representative_point().coords[:])\n",
    "borough_agg['coords'] = [coords[0] for coords in borough_agg['coords']]\n",
    "\n",
    "texts = []\n",
    "for idx, row in borough_agg.iterrows():\n",
    "    texts.append(plt.text(x=row['coords'][0], y=row['coords'][1],\n",
    "                          s=row['NAME'], fontdict={\"color\": \"black\"},\n",
    "                          bbox=dict(facecolor='white', alpha=0.8)))\n",
    "\n",
    "\n",
    "# Resolve overlapping label problem\n",
    "adjust_text(texts, only_move={'points': 'y', 'texts': 'y'}, bbox=dict(\n",
    "    boxstyle='round,pad=0.2', fc='white', alpha=0.8))\n",
    "\n",
    "add_basemap(ax, zoom=11)\n",
    "\n",
    "ax.set_xlabel(\"Easting (WGS84/Pseudo-Mercator)\")\n",
    "ax.set_ylabel(\"Northing\")\n",
    "fig.savefig(\"figures/boro_map.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amb_test.to_pickle(os.path.join(\"pickle\",\"amb_testA.pickle\"))\n",
    "#del(amb_test)\n",
    "\n",
    "#amb_train.to_pickle(os.path.join(\"pickle\",\"amb_trainA.pickle\"))\n",
    "#del(amb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load from pickles\n",
    "amb_test = pd.read_pickle(os.path.join(\"pickle\", \"amb_testA.pickle\"))\n",
    "amb_train = pd.read_pickle(os.path.join(\"pickle\", \"amb_trainA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To adjust fraction of training set for model run \n",
    "sample_slice = 1\n",
    "\n",
    "# Sample for the training set (only relevant if above not set as 1)\n",
    "amb_train_run = amb_train.sample(\n",
    "    n=int(np.round(len(amb_train)*sample_slice, 0)), random_state=100)\n",
    "\n",
    "# Reset the index\n",
    "amb_train_run = amb_train_run.reset_index()\n",
    "(len(amb_train_run)/5)*4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "..................."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model classifers and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer, LabelBinarizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score, cross_validate \n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Create specificity and true negative scores\n",
    "specificity = make_scorer(metrics.recall_score, pos_label=0)\n",
    "true_negative = make_scorer(metrics.precision_score, pos_label=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main classifiers\n",
    "classifiers1 = {\"RF\": RandomForestClassifier(n_estimators=100, \n",
    "                                             random_state=0, \n",
    "                                             min_samples_leaf=100, n_jobs=-1),\n",
    "                \"GBM\": GradientBoostingClassifier(\n",
    "    min_samples_leaf=100, max_depth=100), \n",
    "                \"RF_bal\": RandomForestClassifier(n_estimators=100, \n",
    "                                                 random_state=0, \n",
    "                                                 class_weight=\"balanced\", \n",
    "                                                 min_samples_leaf=100, n_jobs=-1)}\n",
    "\n",
    "# Other classifiers\n",
    "svm = {\"LSV_bal\": SVC(class_weight=\"balanced\",\n",
    "                      kernel=\"linear\", probability=True)}\n",
    "rfmd = {\"RF_bal_md6\": RandomForestClassifier(\n",
    "    n_estimators=100, random_state=0, min_samples_leaf=100, max_depth=6,\n",
    "    n_jobs=-1, class_weight=\"balanced\")}\n",
    "\n",
    "# Define label\n",
    "y_varb = \"conveyed\"\n",
    "\n",
    "#Â Define scoring metrics NB recall = sensitivity\n",
    "scoring = {\"accuracy\": \"accuracy\", \"precision\": \"precision\",\n",
    "           \"roc_auc\": \"roc_auc\",\n",
    "           \"recall\": \"recall\", \"specificity\": specificity,\n",
    "           \"true_negative\": true_negative}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required for intermediate clustering step to work: https://stackoverflow.com/questions/43000969/scikit-learn-kmeans-to-onehot-in-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class KMeans_foo(KMeans):\n",
    "\n",
    "    def fit_transform(self, X, y):\n",
    "        return self.fit_predict(X)\n",
    "\n",
    "    def _transform(self, X, y):\n",
    "        return self.fit_predict(X)\n",
    "\n",
    "\n",
    "class ModelTransformer(TransformerMixin):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return self.model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required for intermediate clustering step to work: https://stackoverflow.com/questions/46162855/fit-transform-takes-2-positional-arguments-but-3-were-given-with-labelbinarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLabelBinarizer(TransformerMixin):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.encoder = LabelBinarizer(*args, **kwargs)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.encoder.fit(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.encoder.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function to incorporate pipelines and fitting of models\n",
    "\n",
    "def process_results_1(key, pipeline, variables1, results, svd):\n",
    "    # Fit model\n",
    "    pipeline.fit(amb_train_run[variables1].values,\n",
    "                 amb_train_run[y_varb].values)\n",
    "\n",
    "    # Feature importances\n",
    "    if svd:\n",
    "        feature_names = np.nan\n",
    "        imps = np.nan\n",
    "        estimator = np.nan\n",
    "    else:\n",
    "        if key == \"LSV_bal\": # LSV has coefs rather than feature imps\n",
    "            feature_names = pipeline.named_steps[\"process\"].named_steps[\"final\"].get_feature_names(\n",
    "            )\n",
    "            zit = pd.Series(pipeline.named_steps[\"classifier\"].coef_[\n",
    "                            :, 0], index=feature_names).sort_values(ascending=False)\n",
    "            imps = zit[0:20].round(2).to_dict()\n",
    "            estimator = np.nan\n",
    "        else:\n",
    "            feature_names = pipeline.named_steps[\"process\"].named_steps[\"final\"].get_feature_names(\n",
    "            )\n",
    "            imps = pd.Series(pipeline.named_steps[\"classifier\"].feature_importances_, index=feature_names).sort_values(\n",
    "                ascending=False)\n",
    "            imps = imps[0:10].round(2).to_dict()\n",
    "\n",
    "            # for Decision tree viz\n",
    "            estimator = pipeline.named_steps[\"classifier\"].estimators_[10]\n",
    "\n",
    "    return feature_names, imps, estimator\n",
    "\n",
    "# Function to incorporate cross validation and results processing\n",
    "def process_results_2(key, pipeline, variables1, variables2, results,\n",
    "                      model, variant, feature_names, imps, estimator):\n",
    "    \n",
    "    # Cross validate model\n",
    "    output = cross_validate(pipeline, amb_train_run[variables1].values,\n",
    "                            amb_train_run[y_varb].values,\n",
    "                            scoring=scoring, cv=5, return_train_score=False,\n",
    "                            return_estimator=True)\n",
    "\n",
    "    mean = output[\"test_roc_auc\"].mean()\n",
    "    std = output[\"test_roc_auc\"].std()\n",
    "    tis = f\" {mean.round(3)} ( {(mean - std*1.96).round(3)} - {(mean + std*1.96).round(3)} )\"\n",
    "\n",
    "    # Predict on test set\n",
    "    pipeline.predict(amb_test[variables1].values)\n",
    "    y_pred = pipeline.predict(amb_test[variables1].values)\n",
    "    y_pred_proba = pipeline.predict_proba(amb_test[variables1].values)\n",
    "\n",
    "    # Metrics\n",
    "    ex_recall = metrics.recall_score(amb_test[y_varb].values, y_pred).round(3)\n",
    "    ex_specificity = specificity(\n",
    "        pipeline, amb_test[variables1].values, amb_test[y_varb].values).round(3)\n",
    "\n",
    "    ex_precision = metrics.precision_score(\n",
    "        amb_test[y_varb].values, y_pred).round(3)\n",
    "    ex_true_negative = true_negative(\n",
    "        pipeline, amb_test[variables1].values, amb_test[y_varb].values).round(3)\n",
    "    ex_roc_auc = metrics.roc_auc_score(\n",
    "        amb_test[y_varb].values, y_pred_proba[:, 1]).round(3)\n",
    "\n",
    "    # IDs of correctly classified\n",
    "    test_predict_bool = y_pred == 0\n",
    "    not_conv_id = amb_test[\"incidentid\"].loc[(\n",
    "        test_predict_bool) & (amb_test.conveyed == 0)].values\n",
    "\n",
    "    # DeLong CI\n",
    "    auc, auc_cov = delong_roc_variance(\n",
    "        amb_test[y_varb].values,\n",
    "        y_pred_proba[:, 1])\n",
    "\n",
    "    alpha = 0.95\n",
    "\n",
    "    auc_std = np.sqrt(auc_cov)\n",
    "    lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "    ci = stats.norm.ppf(lower_upper_q, loc=auc, scale=auc_std)\n",
    "    ci[ci > 1] = 1\n",
    "\n",
    "    tis2 = f\" {ex_roc_auc.round(3)} ( {ci[0].round(3)} - {ci[1].round(3)} )\"\n",
    "    \n",
    "    # List of dictionaries for results tables\n",
    "    results.append({\"Model\": str(model) + str(key), \"Classifier\": key, \"CV recall\": output[\"test_recall\"].mean().round(3),\n",
    "                    \"CV specificity\": output[\"test_specificity\"].mean().round(3),\n",
    "                    \"CV precision\": output[\"test_precision\"].mean().round(3),\n",
    "                    \"CV true negative rate\": output[\"test_true_negative\"].mean().round(3),\n",
    "                    \"CV roc auc\": mean.round(3), \"CV roc auc (CI)\": tis,\n",
    "                    \"Feature Imps\": imps, \"Features\": \", \".join(\n",
    "        variables2), \"Variant\": variant, \"Test precision\": ex_precision,\n",
    "                    \"Test recall\": ex_recall, \"Test specificity\": ex_specificity,\n",
    "                    \"Test true negative rate\": ex_true_negative,\n",
    "                    \"Test roc auc\": ex_roc_auc, \"Test roc auc (CI)\": tis2,\n",
    "                    \"Test y_pred_proba\": y_pred_proba, \"Test y_pred\": y_pred,\n",
    "                    \"Estimator\": estimator,  \"Feature Names\": feature_names,\n",
    "                    \"Test not conveyed IDs\": not_conv_id})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model run functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Run model categorical variable only\n",
    "def modelrun_cat(variables, model, variant, classifiers=classifiers1,\n",
    "                 svd=None, clustering=None):\n",
    "    results = []  # list to contain outputs from each classifier\n",
    "    variables1 = variables\n",
    "    variables2 = variables\n",
    "    if svd:\n",
    "        categories_pipe = Pipeline(steps=[(\"imputer\",\n",
    "                                           SimpleImputer(strategy=\"constant\",\n",
    "                                                         fill_value=\"missing\")), (\n",
    "            \"final\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "                                          (\"truncsvd\", TruncatedSVD(svd))])\n",
    "\n",
    "    else:\n",
    "        categories_pipe = Pipeline(steps=[(\n",
    "            \"imputer\", SimpleImputer(\n",
    "            strategy=\"constant\",\n",
    "                fill_value=\"missing\")), (\"final\",\n",
    "                                           OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "    for key, value in classifiers.items():  # for each classifier\n",
    "        pipeline = Pipeline(\n",
    "            [(\"process\", categories_pipe,), (\"classifier\", classifiers[key]), ])\n",
    "\n",
    "        ret = process_results_1(key, pipeline, variables1, results, svd)\n",
    "        feature_names = ret[0]\n",
    "        imps = ret[1]\n",
    "        estimator = ret[2]\n",
    "\n",
    "        process_results_2(key, pipeline, variables1, variables2,\n",
    "                          results, model, variant, feature_names, imps, estimator)\n",
    "    return pd.DataFrame(results).set_index([\"Model\", \"Features\", \"Variant\", \"Classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Run model with count vectorizer on text data\n",
    "def modelrun_text(text, model, variant, classifiers=classifiers1, svd=None, min_df=10, max_features=500, ngram_range=(1, 2)):\n",
    "    results = []  # list to contain outputs from each classifier\n",
    "    variables1 = text\n",
    "    variables2 = []\n",
    "    variables2.append(text)\n",
    "    if svd:\n",
    "        text_pipe = Pipeline([(\n",
    "            \"final\", CountVectorizer(analyzer=\"word\",\n",
    "                                     token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\",\n",
    "                                                        ngram_range=ngram_range,\n",
    "                                     min_df=min_df, max_features=max_features)),\n",
    "            (\"truncsvd\", TruncatedSVD(svd))])\n",
    "\n",
    "    else:\n",
    "        text_pipe = Pipeline([(\n",
    "            \"final\", CountVectorizer(analyzer=\"word\",\n",
    "                                     token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\",\n",
    "                                                        ngram_range=ngram_range, min_df=min_df, max_features=max_features))])\n",
    "\n",
    "    for key, value in classifiers.items():  # for each classifier\n",
    "        pipeline = Pipeline(\n",
    "            [(\"process\", text_pipe,), (\"classifier\", classifiers[key]), ])\n",
    "\n",
    "        ret = process_results_1(key, pipeline, variables1, results, svd)\n",
    "        feature_names = ret[0]\n",
    "        imps = ret[1]\n",
    "        estimator = ret[2]\n",
    "\n",
    "        process_results_2(key, pipeline, variables1, variables2,\n",
    "                          results, model, variant, feature_names, imps, estimator)\n",
    "    return pd.DataFrame(results).set_index([\"Model\", \"Features\", \"Variant\", \"Classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Run model with TFIDF vectorizer on text data\n",
    "def modelrun_text_tfidf(text, model, variant, classifiers=classifiers1,\n",
    "                        svd=None, min_df=10, ngram_range=(1, 2)):\n",
    "    results = []  # list to contain outputs from each classifier\n",
    "    variables1 = text\n",
    "    variables2 = []\n",
    "    variables2.append(text)\n",
    "    if svd:\n",
    "        text_pipe = Pipeline([(\n",
    "            \"final\", TfidfVectorizer(analyzer=\"word\",\n",
    "                                     token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\",\n",
    "                                                        ngram_range=ngram_range,\n",
    "                                     min_df=min_df)), (\"truncsvd\", TruncatedSVD(svd))])\n",
    "\n",
    "    else:\n",
    "        text_pipe = Pipeline([(\"final\", TfidfVectorizer(\n",
    "            analyzer=\"word\", token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\", ngram_range=ngram_range, min_df=min_df))])\n",
    "\n",
    "    for key, value in classifiers.items():  # for each classifier\n",
    "        pipeline = Pipeline(\n",
    "            [(\"process\", text_pipe,), (\"classifier\", classifiers[key]), ])\n",
    "\n",
    "        ret = process_results_1(key, pipeline, variables1, results, svd)\n",
    "        feature_names = ret[0]\n",
    "        imps = ret[1]\n",
    "        estimator = ret[2]\n",
    "\n",
    "        process_results_2(key, pipeline, variables1, variables2,\n",
    "                          results, model, variant, feature_names, imps, estimator)\n",
    "    return pd.DataFrame(results).set_index([\"Model\", \"Features\", \"Variant\", \"Classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Run model to cluster coordinates\n",
    "def modelrun_num_coord(variables, model, clusters, variant, classifiers=classifiers1):\n",
    "    results = []  # list to contain outputs from each classifier\n",
    "\n",
    "    variables1 = variables\n",
    "    variables2 = variables\n",
    "\n",
    "    cluster_pipe = Pipeline([(\"cluster\", ModelTransformer(\n",
    "        KMeans_foo(clusters, random_state=100))), (\"binary\", MyLabelBinarizer())])\n",
    "\n",
    "    for key, value in classifiers.items():  # for each classifier\n",
    "\n",
    "        pipeline = Pipeline([(\"process\", cluster_pipe,),\n",
    "                             (\"classifier\", classifiers[key]), ])\n",
    "\n",
    "        # Fit model\n",
    "        pipeline.fit(amb_train_run[variables1].values,\n",
    "                     amb_train_run[y_varb].values)\n",
    "\n",
    "        feature_names = list(range(clusters))\n",
    "        imps = pd.Series(pipeline.named_steps[\"classifier\"].feature_importances_,\n",
    "                         index=feature_names).sort_values(ascending=False)\n",
    "        imps = imps[0:20].round(2).to_dict()\n",
    "\n",
    "        estimator = pipeline.named_steps[\"classifier\"].estimators_[9]\n",
    "\n",
    "        process_results_2(key, pipeline, variables1, variables2,\n",
    "                          results, model, variant, feature_names, imps, estimator)\n",
    "\n",
    "    return pd.DataFrame(results).set_index([\"Model\", \"Features\", \"Variant\", \"Classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Run model with numerical data\n",
    "def modelrun_num(variables, model, variant, classifiers=classifiers1):\n",
    "    results = []  # list to contain outputs from each classifier\n",
    "    variables1 = variables\n",
    "    variables2 = variables\n",
    "    numerical_pipe = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer()), (\"scale\", StandardScaler())])\n",
    "\n",
    "    for key, value in classifiers.items():  # for each classifier\n",
    "\n",
    "        if variables[0] != \"dummy0\":\n",
    "            pipeline = Pipeline(\n",
    "                [(\"process\", numerical_pipe,), (\"classifier\", classifiers[key]), ])\n",
    "        else:\n",
    "            pipeline = Pipeline(\n",
    "                [(\"classifier\", classifiers[key]), ])\n",
    "\n",
    "        # Fit model\n",
    "        pipeline.fit(amb_train_run[variables1].values,\n",
    "                     amb_train_run[y_varb].values)\n",
    "\n",
    "        # Feature importances\n",
    "        feature_names = variables\n",
    "        imps = pd.Series(pipeline.named_steps[\"classifier\"].feature_importances_,\n",
    "                         index=feature_names).sort_values(ascending=False)\n",
    "        imps = imps.round(2).to_dict()\n",
    "\n",
    "        estimator = pipeline.named_steps[\"classifier\"].estimators_[9]\n",
    "\n",
    "        process_results_2(key, pipeline, variables1, variables2,\n",
    "                          results, model, variant, feature_names, imps, estimator)\n",
    "    return pd.DataFrame(results).set_index([\"Model\", \"Features\", \"Variant\", \"Classifier\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required to allow extraction of feature names from Pipeline in Column Transformer:\n",
    "https://stackoverflow.com/questions/48005889/get-features-from-sklearn-feature-union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class myPipeline(Pipeline):\n",
    "    def get_feature_names(self):\n",
    "        for name, step in self.steps:\n",
    "            if isinstance(step, CountVectorizer):\n",
    "                return step.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Run model for mixed feature case\n",
    "def modelrun_all_count(model, variant, classifiers=classifiers1, text=None,\n",
    "                       others=None, loc_clusters=None):\n",
    "    variables = []  # list to contain all variables\n",
    "    results = []  # list to contain outputs from each classifier\n",
    "    num_vars = {}  # dict to contain numerical variable positions\n",
    "    cat_vars = {}  # dict to contain cat variables positions\n",
    "    listy = []  # list to contain all elements of main pipeline\n",
    "\n",
    "    numerical_pipe = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer()), (\"scale\", StandardScaler())])\n",
    "\n",
    "    text_pipe = myPipeline([(\"vect\", CountVectorizer(\n",
    "        analyzer=\"word\", token_pattern=r\"(?u)\\b\\w\\w+\\b|!|\\?|\\\"|\\'\")), ])\n",
    "\n",
    "    listy.append((\"text\", text_pipe, 0))\n",
    "    variables.append(text)\n",
    "\n",
    "    variables = variables + others\n",
    "    for i in range(len(others)):\n",
    "        if amb_train_run[others[i]].dtype == \"object\":\n",
    "            cat_vars[others[i]] = i + 1\n",
    "        else:\n",
    "            num_vars[others[i]] = i + 1\n",
    "\n",
    "    categories_pipe = myPipeline(steps=[(\"imputer\", SimpleImputer(\n",
    "        strategy=\"constant\", fill_value=\"missing\")),\n",
    "                                        (\"onehot\", \n",
    "                                         OneHotEncoder(handle_unknown=\"ignore\"))])\n",
    "\n",
    "    if cat_vars:  # if there are categorical features\n",
    "        listy.append((\"categories\", categories_pipe, [*cat_vars.values()]))\n",
    "    if num_vars:  # if there are numerical features\n",
    "        listy.append((\"num\", numerical_pipe, [*num_vars.values()]))\n",
    "\n",
    "    if loc_clusters: # if location cluster set\n",
    "        variables.append(\"lat_incident\")\n",
    "        variables.append(\"lon_incident\")\n",
    "        cluster_pipe = Pipeline([(\"cluster\", ModelTransformer(\n",
    "            KMeans_foo(loc_clusters, random_state=100))),\n",
    "                                 (\"binary\", MyLabelBinarizer())])\n",
    "        j = len(num_vars) + len(cat_vars)\n",
    "        listy.append((\"clust\", cluster_pipe, [j+1, j+2]))\n",
    "\n",
    "    variables1 = variables # Needed to be consistent with other functions\n",
    "    variables2 = variables\n",
    "\n",
    "    for key, value in classifiers.items():  # for each classifier\n",
    "        pipeline = myPipeline(\n",
    "            [(\"union\", ColumnTransformer(\n",
    "                listy, transformer_weights={\"text\": 1, \"num\": 1,\n",
    "                                            \"categories\": 1, \"clust\": 1},)),\n",
    "             (\"classifier\", classifiers[key]), ])\n",
    "\n",
    "        # Fit model\n",
    "        pipeline.fit(amb_train_run[variables].values,\n",
    "                     amb_train_run[y_varb].values)\n",
    "\n",
    "        feature_names1 = pipeline.named_steps[\"union\"].transformers_[\n",
    "            0][1].named_steps[\"vect\"].get_feature_names()\n",
    "\n",
    "        feature_names2 = pipeline.named_steps[\"union\"].transformers_[\n",
    "            1][1].named_steps[\"onehot\"].get_feature_names()\n",
    "\n",
    "        feature_names = feature_names1 + feature_names2.tolist()\n",
    "        feature_names = feature_names + list(num_vars.keys())\n",
    "        \n",
    "        \n",
    "        if loc_clusters: # Use cluster number\n",
    "            feature_names = feature_names + list(range(loc_clusters))\n",
    "        imps = pd.Series(pipeline.named_steps[\"classifier\"].feature_importances_,\n",
    "                         index=feature_names).sort_values(ascending=False)\n",
    "        imps = imps[0:20].round(2).to_dict()\n",
    "\n",
    "        estimator = pipeline.named_steps[\"classifier\"].estimators_[90]\n",
    "\n",
    "        process_results_2(key, pipeline, variables1, variables2,\n",
    "                          results, model, variant, feature_names, imps, estimator)\n",
    "    return pd.DataFrame(results).set_index([\"Model\", \n",
    "                                            \"Features\", \"Variant\", \"Classifier\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeLong method code taken from:\n",
    "    https://github.com/yandexdataschool/roc_comparison/blob/master/compare_auc_delong_xu.py\n",
    "\n",
    "and\n",
    "        \n",
    "https://stackoverflow.com/questions/19124239/scikit-learn-roc-curve-with-confidence-intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     4,
     29,
     53,
     98,
     126
    ]
   },
   "outputs": [],
   "source": [
    "def compute_midrank(x):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and (Z[j] == Z[i]).all():\n",
    "            j += 1\n",
    "        T[i:j] = 0.5*(i + j - 1)\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    # Note(kazeevn) +1 is due to Python using 0-based indexing\n",
    "    # instead of 1-based in the AUC formula in the paper\n",
    "    T2[J] = T + 1\n",
    "    return T2\n",
    "\n",
    "\n",
    "def compute_midrank_weight(x, sample_weight):\n",
    "    \"\"\"Computes midranks.\n",
    "    Args:\n",
    "       x - a 1D numpy array\n",
    "    Returns:\n",
    "       array of midranks\n",
    "    \"\"\"\n",
    "    J = np.argsort(x)\n",
    "    Z = x[J]\n",
    "    cumulative_weight = np.cumsum(sample_weight[J])\n",
    "    N = len(x)\n",
    "    T = np.zeros(N, dtype=np.float)\n",
    "    i = 0\n",
    "    while i < N:\n",
    "        j = i\n",
    "        while j < N and Z[j] == Z[i]:\n",
    "            j += 1\n",
    "        T[i:j] = cumulative_weight[i:j].mean()\n",
    "        i = j\n",
    "    T2 = np.empty(N, dtype=np.float)\n",
    "    T2[J] = T\n",
    "    return T2\n",
    "\n",
    "\n",
    "def fastDeLong(predictions_sorted_transposed, label_1_count):\n",
    "    \"\"\"\n",
    "    The fast version of DeLong's method for computing the covariance of\n",
    "    unadjusted AUC.\n",
    "    Args:\n",
    "       predictions_sorted_transposed: a 2D numpy.array[n_classifiers, n_examples]\n",
    "          sorted such as the examples with label \"1\" are first\n",
    "    Returns:\n",
    "       (AUC value, DeLong covariance)\n",
    "    Reference:\n",
    "     @article{sun2014fast,\n",
    "       title={Fast Implementation of DeLong's Algorithm for\n",
    "              Comparing the Areas Under Correlated Receiver Oerating Characteristic Curves},\n",
    "       author={Xu Sun and Weichao Xu},\n",
    "       journal={IEEE Signal Processing Letters},\n",
    "       volume={21},\n",
    "       number={11},\n",
    "       pages={1389--1393},\n",
    "       year={2014},\n",
    "       publisher={IEEE}\n",
    "     }\n",
    "    \"\"\"\n",
    "    # Short variables are named as they are in the paper\n",
    "    m = label_1_count\n",
    "    n = predictions_sorted_transposed.shape[1] - m\n",
    "    positive_examples = predictions_sorted_transposed[:, :m]\n",
    "    negative_examples = predictions_sorted_transposed[:, m:]\n",
    "    k = predictions_sorted_transposed.shape[0]\n",
    "\n",
    "    tx = np.empty([k, m], dtype=np.float)\n",
    "    ty = np.empty([k, n], dtype=np.float)\n",
    "    tz = np.empty([k, m + n], dtype=np.float)\n",
    "    for r in range(k):\n",
    "        tx[r, :] = compute_midrank(positive_examples[r, :])\n",
    "        ty[r, :] = compute_midrank(negative_examples[r, :])\n",
    "        tz[r, :] = compute_midrank(predictions_sorted_transposed[r, :])\n",
    "    aucs = tz[:, :m].sum(axis=1) / m / n - float(m + 1.0) / 2.0 / n\n",
    "    v01 = (tz[:, :m] - tx[:, :]) / n\n",
    "    v10 = 1.0 - (tz[:, m:] - ty[:, :]) / m\n",
    "    sx = np.cov(v01)\n",
    "    sy = np.cov(v10)\n",
    "    delongcov = sx / m + sy / n\n",
    "    return aucs, delongcov\n",
    "\n",
    "\n",
    "def calc_pvalue(aucs, sigma):\n",
    "    \"\"\"Computes log(10) of p-values.\n",
    "    Args:\n",
    "       aucs: 1D array of AUCs\n",
    "       sigma: AUC DeLong covariances\n",
    "    Returns:\n",
    "       log10(pvalue)\n",
    "    \"\"\"\n",
    "    l = np.array([[1, -1]])\n",
    "    z = np.abs(np.diff(aucs)) / np.sqrt(np.dot(np.dot(l, sigma), l.T))\n",
    "    return np.log10(2) + scipy.stats.norm.logsf(z, loc=0, scale=1) / np.log(10)\n",
    "\n",
    "\n",
    "def compute_ground_truth_statistics(ground_truth, sample_weight=None):\n",
    "    assert np.array_equal(np.unique(ground_truth), [0, 1])\n",
    "    order = (-ground_truth).argsort()\n",
    "    label_1_count = int(ground_truth.sum())\n",
    "    if sample_weight is None:\n",
    "        ordered_sample_weight = None\n",
    "    else:\n",
    "        ordered_sample_weight = sample_weight[order]\n",
    "\n",
    "    return order, label_1_count  # ordered_sample_weight\n",
    "\n",
    "\n",
    "def delong_roc_variance(ground_truth, predictions):\n",
    "    \"\"\"\n",
    "    Computes ROC AUC variance for a single set of predictions\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions: np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    sample_weight = None\n",
    "    order, label_1_count = compute_ground_truth_statistics(\n",
    "        ground_truth, sample_weight)\n",
    "    predictions_sorted_transposed = predictions[np.newaxis, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
    "    assert len(\n",
    "        aucs) == 1, \"There is a bug in the code, please forward this to the developers\"\n",
    "    return aucs[0], delongcov\n",
    "\n",
    "\n",
    "def delong_roc_test(ground_truth, predictions_one, predictions_two):\n",
    "    \"\"\"\n",
    "    Computes log(p-value) for hypothesis that two ROC AUCs are different\n",
    "    Args:\n",
    "       ground_truth: np.array of 0 and 1\n",
    "       predictions_one: predictions of the first model,\n",
    "          np.array of floats of the probability of being class 1\n",
    "       predictions_two: predictions of the second model,\n",
    "          np.array of floats of the probability of being class 1\n",
    "    \"\"\"\n",
    "    sample_weight = None\n",
    "    order, label_1_count = compute_ground_truth_statistics(ground_truth)\n",
    "    predictions_sorted_transposed = np.vstack(\n",
    "        (predictions_one, predictions_two))[:, order]\n",
    "    aucs, delongcov = fastDeLong(predictions_sorted_transposed, label_1_count)\n",
    "    return calc_pvalue(aucs, delongcov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_baseline = modelrun_cat(\n",
    "    variables=[\"dohcategory\"], model=\"Baseline\", variant=\"NaN\")\n",
    "results_baseline.to_pickle(os.path.join(\"pickle\", \"results_baselineA.pickle\"))\n",
    "results_baseline = pd.read_pickle(\n",
    "    os.path.join(\"pickle\", \"results_baselineA.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A (MPDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mpds = modelrun_cat(variables=[\"ampdscode\"], model=\"A1\", variant=\"NaN\")\n",
    "results_mpds.to_pickle(os.path.join(\"pickle\", \"results_mpdsA.pickle\"))\n",
    "results_mpds = pd.read_pickle(os.path.join(\"pickle\", \"results_mpdsA.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B (free text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_text = modelrun_text(\n",
    "    text=\"prob_desc\", model=\"B1\", variant=\"NaN\", svd=None, ngram_range=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_text.to_csv(\"results_text.csv\")\n",
    "results_text.to_pickle(os.path.join(\"pickle\", \"results_textA.pickle\"))\n",
    "results_text = pd.read_pickle(os.path.join(\"pickle\", \"results_textA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_text(text=\"prob_desc\", model=\"B1\", classifiers=rfmd,\n",
    "                   variant=\"NaN\", svd=None, ngram_range=(1, 1))\n",
    "results_text.append(z1).to_csv(\"results_text.csv\")\n",
    "results_text.append(z1).to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_textA.pickle\"))\n",
    "results_text = pd.read_pickle(os.path.join(\"pickle\", \"results_textA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_text(text=\"prob_desc\", model=\"B2\",\n",
    "                   variant=\"ngram_range=(1,2)\", svd=None, ngram_range=(1, 2))\n",
    "results_text.append(z1).to_csv(\"results_text.csv\")\n",
    "results_text.append(z1).to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_textA.pickle\"))\n",
    "results_text = pd.read_pickle(os.path.join(\"pickle\", \"results_textA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_text(text=\"prob_desc\", model=\"B3\",\n",
    "                   variant=\"ngram_range=(1,3)\", svd=None, ngram_range=(1, 3))\n",
    "results_text.append(z1).to_csv(\"results_text.csv\")\n",
    "results_text.append(z1).to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_textA.pickle\"))\n",
    "results_text = pd.read_pickle(os.path.join(\"pickle\", \"results_textA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_text(text=\"prob_desc_lemmat\", model=\"B4\",\n",
    "                   variant=\"lemmatized text\", svd=None, ngram_range=(1, 1))\n",
    "results_text.append(z1).to_csv(\"results_text.csv\")\n",
    "results_text.append(z1).to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_textA.pickle\"))\n",
    "results_text = pd.read_pickle(os.path.join(\"pickle\", \"results_textA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_text(text=\"prob_desc_no_punc\", model=\"B5\",\n",
    "                   variant=\"no punctuation\", svd=None, ngram_range=(1, 1))\n",
    "results_text.append(z1).to_csv(\"results_text.csv\")\n",
    "results_text.append(z1).to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_textA.pickle\"))\n",
    "results_text = pd.read_pickle(os.path.join(\"pickle\", \"results_textA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_text(text=\"rake_key_words\", model=\"B6\",\n",
    "                   variant=\"rake, ngram_range=(1,1)\", svd=None, ngram_range=(1, 1))\n",
    "\n",
    "results_text.append(z1).to_csv(\"results_text.csv\")\n",
    "results_text.append(z1).to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_textA.pickle\"))\n",
    "results_text = pd.read_pickle(os.path.join(\"pickle\", \"results_textA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_text_tfidf(text=\"prob_desc\", model=\"B7\",\n",
    "                         variant=\"TF-IDF\", svd=None, ngram_range=(1, 1))\n",
    "results_text.append(z1).to_csv(\"results_text.csv\")\n",
    "results_text.append(z1).to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_textA.pickle\"))\n",
    "results_text = pd.read_pickle(os.path.join(\"pickle\", \"results_textA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_text(text=\"prob_desc\", model=\"B8\", variant=\"maximum_features=100\",\n",
    "                   max_features=100, svd=None, ngram_range=(1, 1))\n",
    "results_text.append(z1).to_csv(\"results_text.csv\")\n",
    "results_text.append(z1).to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_textA.pickle\"))\n",
    "results_text = pd.read_pickle(os.path.join(\"pickle\", \"results_textA.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C (locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_loc = modelrun_cat([\"coords_text\"], model=\"C1\", variant=\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_loc.to_csv(\"results_loc.csv\")\n",
    "results_loc.to_pickle(os.path.join(\"pickle\", \"results_locA.pickle\"))\n",
    "results_loc = pd.read_pickle(os.path.join(\"pickle\", \"results_locA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_cat([\"msoa11nm\"], model=\"C2\", variant=\"NaN\")\n",
    "results_loc.append(z1).to_csv(\"results_loc.csv\")\n",
    "results_loc.append(z1).to_pickle(os.path.join(\"pickle\", \"results_locA.pickle\"))\n",
    "results_loc = pd.read_pickle(os.path.join(\"pickle\", \"results_locA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_num_coord(variables=[\"lat_incident\", \"lon_incident\"],\n",
    "                        model=\"C3\", clusters=250, variant=\"KMeans(k=250) on lat,lon\")\n",
    "results_loc.append(z1).to_csv(\"results_loc.csv\")\n",
    "results_loc.append(z1).to_pickle(os.path.join(\"pickle\", \"results_locA.pickle\"))\n",
    "results_loc = pd.read_pickle(os.path.join(\"pickle\", \"results_locA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_num_coord(variables=[\"lat_incident\", \"lon_incident\"],\n",
    "                        model=\"C4\", clusters=100, variant=\"KMeans(k=100) on lat,lon\")\n",
    "results_loc.append(z1).to_csv(\"results_loc.csv\")\n",
    "results_loc.append(z1).to_pickle(os.path.join(\"pickle\", \"results_locA.pickle\"))\n",
    "results_loc = pd.read_pickle(os.path.join(\"pickle\", \"results_locA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_num_coord(variables=[\"lat_incident\", \"lon_incident\"],\n",
    "                        model=\"C5\", clusters=50, variant=\"KMeans(k=50) on lat,lon\")\n",
    "results_loc.append(z1).to_csv(\"results_loc.csv\")\n",
    "results_loc.append(z1).to_pickle(os.path.join(\"pickle\", \"results_locA.pickle\"))\n",
    "results_loc = pd.read_pickle(os.path.join(\"pickle\", \"results_locA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "z1 = modelrun_cat([\"borough\"], model=\"C6\", variant=\"NaN\")\n",
    "results_loc.append(z1).to_csv(\"results_loc.csv\")\n",
    "results_loc.append(z1).to_pickle(os.path.join(\"pickle\", \"results_locA.pickle\"))\n",
    "results_loc = pd.read_pickle(os.path.join(\"pickle\", \"results_locA.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D (Numerical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_num = modelrun_num([\"age\"], model=\"D1\", variant=\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_num.to_csv(\"results_num.csv\")\n",
    "results_num.to_pickle(os.path.join(\"pickle\", \"results_numA.pickle\"))\n",
    "results_num = pd.read_pickle(os.path.join(\"pickle\", \"results_numA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_num([\"cos_hour\", \"sin_hour\"], model=\"D2\", variant=\"NaN\")\n",
    "results_num.append(z1).to_csv(\"results_num.csv\")\n",
    "results_num.append(z1).to_pickle(os.path.join(\"pickle\", \"results_numA.pickle\"))\n",
    "results_num = pd.read_pickle(os.path.join(\"pickle\", \"results_numA.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E (Mixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "results_mixed = modelrun_all_count(text=\"prob_desc\", others=[\n",
    "                                   \"ampdscode\"], model=\"E1\", variant=\"NaN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_mixed.to_csv(\"results_mixed.csv\")\n",
    "results_mixed.to_pickle(os.path.join(\"pickle\", \"results_mixedA.pickle\"))\n",
    "results_mixed = pd.read_pickle(os.path.join(\"pickle\", \"results_mixedA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_all_count(text=\"prob_desc\", others=[\n",
    "                        \"ampdscode\"],\n",
    "                        loc_clusters=50, model=\"E2\", variant=\"KMeans(k=50) on lat,lon\")\n",
    "results_mixed.append(z1).to_csv(\"results_mixed.csv\")\n",
    "results_mixed.append(z1).to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_mixedA.pickle\"))\n",
    "results_mixed = pd.read_pickle(os.path.join(\"pickle\", \"results_mixedA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_all_count(text=\"prob_desc\", others=[\n",
    "                        \"ampdscode\", \"msoa11nm\"], model=\"E3\", variant=\"NaN\")\n",
    "results_mixed.append(z1).to_csv(\"results_mixed.csv\")\n",
    "results_mixed.append(z1).to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_mixedA.pickle\"))\n",
    "results_mixed = pd.read_pickle(os.path.join(\"pickle\", \"results_mixedA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_all_count(text=\"prob_desc\", others=[\n",
    "                        \"ampdscode\", \"age\"],\n",
    "                        loc_clusters=100, model=\"E4\", variant=\"KMeans(k=100) on lat,lon\")\n",
    "results_mixed.append(z1).to_csv(\"results_mixed.csv\")\n",
    "results_mixed.append(z1).to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_mixedA.pickle\"))\n",
    "results_mixed = pd.read_pickle(os.path.join(\"pickle\", \"results_mixedA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_all_count(text=\"prob_desc\", others=[\n",
    "                                   \"ampdscode\", \"age\"], model=\"E5\", variant=\"NaN\")\n",
    "results_mixed.append(z1).to_csv(\"results_mixed.csv\")\n",
    "results_mixed.append(z1).to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_mixedA.pickle\"))\n",
    "results_mixed = pd.read_pickle(os.path.join(\"pickle\", \"results_mixedA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_all_count(text=\"prob_desc\", others=[\n",
    "                        \"ampdscode\", \"age\", \"cos_hour\", \"sin_hour\"], loc_clusters=100, model=\"E6\", variant=\"KMeans(k=100) on lat,lon\")\n",
    "results_mixed.append(z1).to_csv(\"results_mixed.csv\")\n",
    "results_mixed.append(z1).to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_mixedA.pickle\"))\n",
    "results_mixed = pd.read_pickle(os.path.join(\"pickle\", \"results_mixedA.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra text runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_text(text=\"prob_desc\", model=\"B8\", classifiers=svm,\n",
    "                   variant=\"max_features=100\", \n",
    "                   max_features=100, svd=None, ngram_range=(1, 1))\n",
    "results_text.append(z1).to_csv(\"results_text.csv\")\n",
    "results_text.append(z1).to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_textA.pickle\"))\n",
    "results_text = pd.read_pickle(os.path.join(\"pickle\", \"results_textA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1 = modelrun_text(text=\"prob_desc\", model=\"B9\",\n",
    "                   variant=\"svd=100\", svd=100, ngram_range=(1, 1))\n",
    "results_text.append(z1).to_csv(\"results_text.csv\")\n",
    "results_text.append(z1).to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_textA.pickle\"))\n",
    "results_text = pd.read_pickle(os.path.join(\"pickle\", \"results_textA.pickle\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results (RQ 1-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_baseline = pd.read_pickle(\n",
    "    os.path.join(\"pickle\", \"results_baselineA.pickle\"))\n",
    "results_mpds = pd.read_pickle(os.path.join(\"pickle\", \"results_mpdsA.pickle\"))\n",
    "results_text = pd.read_pickle(os.path.join(\"pickle\", \"results_textA.pickle\"))\n",
    "results_loc = pd.read_pickle(os.path.join(\"pickle\", \"results_locA.pickle\"))\n",
    "results_num = pd.read_pickle(os.path.join(\"pickle\", \"results_numA.pickle\"))\n",
    "results_mixed = pd.read_pickle(os.path.join(\"pickle\", \"results_mixedA.pickle\"))\n",
    "\n",
    "# Create results table with all model runs\n",
    "results_all = results_baseline.append(results_mpds).append(\n",
    "    results_text).append(results_loc).append(results_num).append(results_mixed)\n",
    "results_all.to_csv(\"results_all.csv\")\n",
    "results_all.to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_allA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all = pd.read_pickle(os.path.join(\"pickle\", \"results_allA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to recalculate DeLong CIs but with Bonferonni correction\n",
    "def adjust_ci(x):    \n",
    "    auc, auc_cov = delong_roc_variance(\n",
    "        amb_test[y_varb].values,\n",
    "        x[0][:, 1])\n",
    "    \n",
    "    alpha = 1 - (0.05/len(results_all))\n",
    "\n",
    "    auc_std = np.sqrt(auc_cov)\n",
    "    lower_upper_q = np.abs(np.array([0, 1]) - (1 - alpha) / 2)\n",
    "\n",
    "    ci = stats.norm.ppf(lower_upper_q, loc=auc, scale=auc_std)\n",
    "    ci[ci > 1] = 1\n",
    "\n",
    "    return f\" {x[1]} ({ci[0].round(3)} - {ci[1].round(3)})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column that has both the ROC-AUC point estimate and predicted probabilities\n",
    "results_all['adjust_roc'] = results_all.apply(\n",
    "    lambda row: [row[\"Test y_pred_proba\"],row[\"Test roc auc\"]], axis = 1) \n",
    "results_all[\"Test roc auc (CI)\"] = results_all['adjust_roc'].apply(lambda x: adjust_ci(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Function to run DeLong t test against baseline ROC-AUC\n",
    "def roc_auc_test2(x):\n",
    "    ground_truth = amb_test[y_varb].values\n",
    "    predictions1 = results_all[\"Test y_pred_proba\"].iloc[0][:, 1]\n",
    "    predictions2 = x[:, 1]\n",
    "    k = np.exp(delong_roc_test(ground_truth=ground_truth,\n",
    "                               predictions_one=predictions1, \n",
    "                               predictions_two=predictions2))\n",
    "    return k[0][0]\n",
    "\n",
    "results_all[\"p value\"] = results_all[\"Test y_pred_proba\"].apply(\n",
    "    lambda x: roc_auc_test2(x))\n",
    "\n",
    "\n",
    "# Limit p value display\n",
    "def stat_sig_2(x):\n",
    "    if ~np.isnan(x):\n",
    "        if x < 0.01/ (len(results_all) - 3):\n",
    "            return \"<\"+str(np.round(0.01/(len(results_all) - 3), 5))\n",
    "        else: \n",
    "            return np.round(x, 5)\n",
    "    else:\n",
    "        return \"-\"\n",
    "\n",
    "results_all[\"p-value\"] = results_all[\"p value\"].apply(lambda x: stat_sig_2(x))\n",
    "results_all.to_pickle(\n",
    "    os.path.join(\"pickle\", \"results_allA.pickle\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_1 = results_all.iloc[0:35]\n",
    "results_2 = results_all.iloc[35:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossval_tables = [\"CV recall\", \"CV specificity\",\n",
    "                   \"CV precision\", \"CV true negative rate\", \"CV roc auc\"]\n",
    "\n",
    "# Shorter names\n",
    "crossval_columns = [\"Recall\", \"Specificity\",\n",
    "                   \"Precision\", \"TN rate\", \"ROC-AUC\"]\n",
    "\n",
    "test_tables = [\"Test recall\", \"Test specificity\", \"Test precision\",\n",
    "               \"Test true negative rate\", \"Test roc auc (CI)\", \"p-value\"]\n",
    "\n",
    "# Shorter names\n",
    "test_columns = [\"Recall\", \"Specificity\",\n",
    "                   \"Precision\", \"TN rate\", \"ROC-AUC (CI)\", \"p-value\"]\n",
    "\n",
    "# Later changed in latex doc to even shorter through find and replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First appendix results table\n",
    "\n",
    "# Cross validation\n",
    "results_1_cv = results_1[crossval_tables]\n",
    "results_1_cv.columns = crossval_columns\n",
    "results_1_cv = pd.concat([results_1_cv], keys=[\"CV\"], names = \"\", axis=1)\n",
    "\n",
    "# Testing\n",
    "results_1_test = results_1[test_tables]\n",
    "results_1_test.columns = test_columns\n",
    "results_1_test = pd.concat([results_1_test], keys=[\"Testing\"], names = \"\", axis=1)\n",
    "\n",
    "results_1_final  = pd.concat([results_1_cv, results_1_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Results table for report\n",
    "print(results_1_final.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As above for rest of model results\n",
    "results_2_cv = results_2[crossval_tables]\n",
    "results_2_cv.columns = crossval_columns\n",
    "results_2_cv = pd.concat([results_2_cv], keys=[\"CV\"], names = \"\", axis=1)\n",
    "\n",
    "results_2_test = results_2[test_tables]\n",
    "results_2_test.columns = test_columns\n",
    "results_2_test = pd.concat([results_2_test], keys=[\"Testing\"], names = \"\", axis=1)\n",
    "\n",
    "results_2_final  = pd.concat([results_2_cv, results_2_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_2_final.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models to display in body of text\n",
    "\n",
    "intext_models1 = [\"BaselineRF\", \"BaselineGBM\", \"BaselineRF_bal\", \"A1RF\", \"A1GBM\",\n",
    "\"A1RF_bal\", \"B1RF\", \"B1GBM\", \"B1RF_bal\", \"B1RF_bal_md6\"]\n",
    "\n",
    "intext_models2 = [\"C4RF\", \"C4GBM\", \"C4RF_bal\", \"D1RF\",\n",
    "\"D1GBM\", \"D1RF_bal\", \"D2RF\", \"D2GBM\", \"D2RF_bal\", \"E1RF\" \"E1GBM\", \"E6RF\", \"E6GBM\", \"E6RF_bal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline, MPDS and text\n",
    "\n",
    "results_sel1 = results_all.reset_index()\n",
    "\n",
    "results_sel1 = results_sel1.loc[results_sel1.Model.isin(intext_models1)].set_index(\"Model\")\n",
    "\n",
    "results_sel1_cv = results_sel1[crossval_tables]\n",
    "results_sel1_cv.columns = crossval_columns\n",
    "results_sel1_cv = pd.concat([results_sel1_cv], keys=[\"CV\"], names = \"\", axis=1)\n",
    "\n",
    "results_sel1_test = results_sel1[test_tables]\n",
    "results_sel1_test.columns = test_columns\n",
    "results_sel1_test = pd.concat([results_sel1_test], keys=[\"Testing\"], names = \"\", axis=1)\n",
    "\n",
    "results_sel1_final  = pd.concat([results_sel1_cv, results_sel1_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results_sel1_final.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other models\n",
    "\n",
    "results_sel2 = results_all.reset_index()\n",
    "\n",
    "results_sel2 = results_sel2.loc[results_sel2.Model.isin(intext_models2)].set_index(\"Model\")\n",
    "\n",
    "results_sel2_cv = results_sel2[crossval_tables]\n",
    "results_sel2_cv.columns = crossval_columns\n",
    "results_sel2_cv = pd.concat([results_sel2_cv], keys=[\"CV\"], names = \"\", axis=1)\n",
    "\n",
    "results_sel2_test = results_sel2[test_tables]\n",
    "results_sel2_test.columns = test_columns\n",
    "results_sel2_test = pd.concat([results_sel2_test], keys=[\"Testing\"], names = \"\", axis=1)\n",
    "\n",
    "results_sel2_final  = pd.concat([results_sel2_cv, results_sel2_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(results_sel2_final.to_latex(index=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scikitplot as skplt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted probabilities\n",
    "probas_list = [results_all[\"Test y_pred_proba\"][1],\n",
    "               results_all[\"Test y_pred_proba\"][6],\n",
    "               results_all[\"Test y_pred_proba\"][7],\n",
    "               results_all[\"Test y_pred_proba\"][8], \n",
    "               results_all[\"Test y_pred_proba\"][41],\n",
    "               results_all[\"Test y_pred_proba\"][50],\n",
    "               results_all[\"Test y_pred_proba\"][71]]\n",
    "\n",
    "# Model/classifier names\n",
    "clf_names = [results_all.reset_index()[\"Model\"][1],\n",
    "             results_all.reset_index()[\"Model\"][6],\n",
    "             results_all.reset_index()[\"Model\"][7],\n",
    "             results_all.reset_index()[\"Model\"][8],\n",
    "             results_all.reset_index()[\"Model\"][41],\n",
    "             results_all.reset_index()[\"Model\"][50],\n",
    "             results_all.reset_index()[\"Model\"][71]]\n",
    "\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Calibration curve\n",
    "skplt.metrics.plot_calibration_curve(\n",
    "    amb_test[y_varb].values, probas_list, clf_names, ax=ax)\n",
    "fig.savefig(\"figures/calibplot.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characteristics of  (RQ3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest tree viz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph viz export of model B1RF_bal_md6\n",
    "\n",
    "export_graphviz(results_all[\"Estimator\"][9], out_file=\"figures/tree.dot\",\n",
    "                feature_names=z1[\"Feature Names\"][0], class_names=[\n",
    "                \"Conveyed\", \"Not Conveyed\"], rounded=True,\n",
    "                proportion=False, precision=2, filled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# Get features from results tables and sort in descending order or importance\n",
    "pd.Series(results_text[\"Feature Imps\"][1]).sort_values(\n",
    "    ascending=True).plot.barh(color=green, ax=ax)\n",
    "ax.set_xlabel(\"Importance\")\n",
    "ax.set_ylabel(\"Feature\")\n",
    "fig.savefig(\"figures/imp_text.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusters for models C3, E5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goeg = gpd.GeoDataFrame(amb_train_run.dropna(\n",
    "    subset=[\"geometry\"]), geometry=\"geometry\")\n",
    "goeg.crs = {'init' :'epsg:4326'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = pd.DataFrame(KMeans(n_clusters=100, random_state=100).fit(\n",
    "    goeg[[\"lat_incident\", \"lon_incident\"]].values).cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create normal voronoi\n",
    "vor = Voronoi(np.c_[arr[1], arr[0]])\n",
    "\n",
    "# Finite region voronoi\n",
    "regions, vertices = voronoi_finite_polygons_2d(vor)\n",
    "poly = geometry.Polygon(vertices)\n",
    "\n",
    "# Create list of vertices in each polygon\n",
    "cells = [vertices[region] for region in regions]\n",
    "listy = []\n",
    "for cell in cells:\n",
    "    polygon = Polygon(cell)\n",
    "    listy.append(polygon)\n",
    "\n",
    "# New column in data frame with geometry\n",
    "arr[\"geometry\"] = listy\n",
    "arr = gpd.GeoDataFrame(\n",
    "    arr, geometry=\"geometry\", crs={\"init\": \"epsg:4326\"})\n",
    "\n",
    "# Use merged London borough boundaries to cut polygons\n",
    "arr = gpd.overlay(arr, london, how=\"intersection\")\n",
    "\n",
    "# Convert crs\n",
    "arr = arr.to_crs(epsg=3857)\n",
    "\n",
    "# Make figure\n",
    "fig, ax = plt.subplots(figsize=(15, 10.5))\n",
    "arr.plot(ax=ax, cmap=\"viridis\", alpha=0.8)\n",
    "\n",
    "# North arrow\n",
    "ax.text(30000, 6680000, '\\u25B2 \\nN ', ha='center', fontsize=40,\n",
    "        family='Arial', bbox=dict(facecolor='white', alpha=0.3))\n",
    "\n",
    "# Basemap\n",
    "add_basemap(ax, zoom=11)\n",
    "\n",
    "ax.set_xlabel(\"Easting (WGS84/Pseudo-Mercator)\")\n",
    "ax.set_ylabel(\"Northing\")\n",
    "fig.savefig(\"figures/clus_map.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conveyance rate and occurance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding manual text features to data set\n",
    "\n",
    "amb_train[\"(nfda)\"] = amb_train.prob_desc.apply(\n",
    "    lambda x: 1 if re.search(r\"(nfa)|(nfda)|(homeless)\", x) else 0)\n",
    "\n",
    "amb_train[\"?\"] = amb_train.prob_desc.apply(\n",
    "    lambda x: 1 if re.search(r\"(\\?)\", x) else 0)\n",
    "\n",
    "amb_train[\"(alc)\"] = amb_train.prob_desc.apply(lambda x: 1 if re.search(\n",
    "    r\"(intoxicated)|(intox)|(alcohol)|(alc)|(drunk)|(alcoholic)\", x) else 0)\n",
    "\n",
    "amb_test[\"(nfda)\"] = amb_test.prob_desc.apply(\n",
    "    lambda x: 1 if re.search(r\"(nfa)|(nfda)|(homeless)\", x) else 0)\n",
    "\n",
    "amb_test[\"?\"] = amb_test.prob_desc.apply(\n",
    "    lambda x: 1 if re.search(r\"(\\?)\", x) else 0)\n",
    "\n",
    "amb_test[\"(alc)\"] = amb_test.prob_desc.apply(lambda x: 1 if re.search(\n",
    "    r\"(intoxicated)|(intox)|(alcohol)|(alc)|(drunk)|(alcoholic)\", x) else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"Skinny fries\" chart\n",
    "sns.set_style(\"dark\")\n",
    "amb_train[\"(ALL INCS)\"] = 1\n",
    "pre_made = [\"(nfda)\", \"?\", \"(alc)\", \"(ALL INCS)\"]\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "vabs = [\"heart\", \"hx\", \"pain\", \"head\", \"fall\", \"dob\", \"dizzy\", \"and\", \"not\",\n",
    "        \"pat\", \"on\", \"(ALL INCS)\", \"ons\", \"?\", \"(alc)\", \"uncon\", \"caller\", \"(nfda)\"]\n",
    "\n",
    "# Fix for spacing\n",
    "ind = np.arange(len(vabs))*0.5\n",
    "ind[10:] = ind[10:]+0.25\n",
    "ind[11:] = ind[11:]+0.5\n",
    "ind[12:] = ind[12:]+0.5\n",
    "ind[13:] = ind[13:]+0.25\n",
    "\n",
    "conv = []\n",
    "width = [] # bar width\n",
    "\n",
    "# Iterate through all of the terms to get conveyance rate and frequency of occurrence\n",
    "for i in range(len(vabs)):\n",
    "    if vabs[i] not in pre_made:\n",
    "        my_regex = r\"(\" + re.escape(vabs[i]) + r\")\"\n",
    "        amb_train[vabs[i]] = amb_train.prob_desc.apply(\n",
    "            lambda x: 1 if re.search(my_regex, x) else 0)\n",
    "\n",
    "    conv.append(len(amb_train.loc[(amb_train[vabs[i]] == 1) & (\n",
    "        amb_train.conveyed == 1)])*100/len(amb_train[amb_train[vabs[i]] == 1]))\n",
    "\n",
    "    width.append(amb_train[vabs[i]].sum()/len(amb_train))\n",
    "\n",
    "p1 = plt.bar(ind, conv, width=width, edgecolor=green, color=green)\n",
    "p3 = plt.plot(0.7)\n",
    "plt.xticks(ind, vabs)\n",
    "plt.ylabel(\"% conveyed\")\n",
    "plt.xlabel(\"Term (bar width proportional to rate of occurence)\")\n",
    "plt.savefig(\"figures/skinny.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call category distribution and terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viridis colors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "cmap = matplotlib.cm.get_cmap('viridis')\n",
    "\n",
    "rgb1 = cmap(0.1)\n",
    "rgb2 = cmap(0.3)\n",
    "rgb3 = cmap(0.5)\n",
    "rgb4 = cmap(0.7)\n",
    "rgb5 = cmap(0.9)\n",
    "\n",
    "# Call cats for all incidents \n",
    "all_cats = pd.DataFrame(amb_train.groupby(\n",
    "    by=\"dohcategory\").count()[\"incidentid\"]).apply(\n",
    "    lambda x: x*100/x.sum()).rename(\n",
    "    columns={\"incidentid\": \"Proportion of calls\"})\n",
    "\n",
    "# Call cats for incidents with nfda in free text\n",
    "nfda_cats = pd.DataFrame(amb_train.loc[amb_train[\"(nfda)\"] == 1].groupby(\n",
    "    by=\"dohcategory\").count()[\n",
    "                         \"incidentid\"]).apply(\n",
    "    lambda x: x*100/x.sum()).rename(columns={\"incidentid\": \"Proportion of calls\"})\n",
    "\n",
    "\n",
    "# Call cats for incidents with alcohol in free text\n",
    "alc1_cats = pd.DataFrame(amb_train.loc[amb_train[\"(alc)\"] == 1].groupby(\n",
    "    by=\"dohcategory\").count()[\n",
    "                         \"incidentid\"]).apply(lambda x: x*100/x.sum()).rename(\n",
    "    columns={\"incidentid\": \"Proportion of calls\"})\n",
    "\n",
    "# Call cats for incidents with hx in free text\n",
    "hx_cats = pd.DataFrame(amb_train.loc[amb_train.hx == 1].groupby(\n",
    "    by=\"dohcategory\").count()[\n",
    "                       \"incidentid\"]).apply(lambda x: x*100/x.sum()).rename(\n",
    "    columns={\"incidentid\": \"Proportion of calls\"})\n",
    "\n",
    "# Call cats for incidents with pain in free text\n",
    "pain_cats = pd.DataFrame(amb_train.loc[amb_train.pain == 1].groupby(\n",
    "    by=\"dohcategory\").count()[\n",
    "                         \"incidentid\"]).apply(lambda x: x*100/x.sum()).rename(\n",
    "    columns={\"incidentid\": \"Proportion of calls\"})\n",
    "\n",
    "\n",
    "N = 5 # Number of all categories for colors\n",
    "ind = np.arange(N)\n",
    "# Array of C1s\n",
    "C1 = np.array([alc1_cats.iloc[0][0], nfda_cats.iloc[0][0], \n",
    "      all_cats.iloc[0][0], hx_cats.iloc[0][0], pain_cats.iloc[0][0]])\n",
    "C2 =  np.array([alc1_cats.iloc[1][0], nfda_cats.iloc[1][0],\n",
    "      all_cats.iloc[1][0], hx_cats.iloc[1][0], pain_cats.iloc[1][0]])\n",
    "\n",
    "C3 =  np.array([alc1_cats.iloc[2][0], nfda_cats.iloc[2][0],\n",
    "      all_cats.iloc[2][0], hx_cats.iloc[2][0], pain_cats.iloc[2][0]])\n",
    "\n",
    "C4 =  np.array([alc1_cats.iloc[3][0], nfda_cats.iloc[3][0],\n",
    "      all_cats.iloc[3][0], hx_cats.iloc[3][0], pain_cats.iloc[3][0]])\n",
    "\n",
    "C5 =  np.array([alc1_cats.iloc[4][0], nfda_cats.iloc[4][0],\n",
    "      all_cats.iloc[4][0], hx_cats.iloc[4][0], pain_cats.iloc[4][0]])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "#Plot each call category in turn overlaying on top of other\n",
    "p1 = plt.bar(ind, C1, color=rgb1)\n",
    "p2 = plt.bar(ind, C2, bottom = C1, color=rgb2)\n",
    "p3 = plt.bar(ind, C3, bottom = C1 + C2, color=rgb3)\n",
    "p4 = plt.bar(ind, C4, bottom = C1 + C2 + C3, color=rgb4)\n",
    "p5 = plt.bar(ind, C5, bottom = C1 + C2 + C3 + C4, color=rgb5)\n",
    "\n",
    "plt.legend((p1[0], p2[0], p3[0], p4[0], p5[0]), ('C1', 'C2', 'C3', 'C4', 'C5'))\n",
    "plt.xticks(ind, ('(alc)', '(nfda)', '(ALL INCS)', 'hx', 'pain'))\n",
    "plt.ylabel(\"% of incidents\")\n",
    "plt.xlabel(\"Term\")\n",
    "plt.savefig(\"figures/priority_ft.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctly classified as not conveyed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all.iloc[75,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_class = amb_test.loc[(amb_test.incidentid.isin(\n",
    "    results_all.iloc[75][\"Test not conveyed IDs\"])) & (amb_test.conveyed == 0)]\n",
    "len(correct_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_class = amb_test.loc[(amb_test.incidentid.isin(\n",
    "    results_all.iloc[75][\"Test not conveyed IDs\"])) & (amb_test[\"(nfda)\"] == 1)\n",
    "                            & (amb_test.conveyed == 0)]\n",
    "len(correct_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_all.iloc[7,:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_class = amb_test.loc[(amb_test.incidentid.isin(\n",
    "    results_all.iloc[7][\"Test not conveyed IDs\"])) & (amb_test.conveyed == 0)]\n",
    "len(correct_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_class = amb_test.loc[(amb_test.incidentid.isin(\n",
    "    results_all.iloc[7][\"Test not conveyed IDs\"])) & (amb_test[\"(nfda)\"] == 1)\n",
    "                            & (amb_test.conveyed == 0)]\n",
    "len(correct_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concordance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nltk text object with whole \"corpus\"\n",
    "all_free_text = amb_train.prob_desc.to_string(index=False)\n",
    "all_free_text = nltk.word_tokenize(all_free_text)\n",
    "text_ob = nltk.Text(all_free_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ob.concordance(\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ob.concordance(\"uncon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ob.concordance(\"caller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ob.concordance(\"pain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ob.concordance(\"head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_ob.concordance(\"heart\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package version numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "packages = [\"numpy\", \"pandas\", \"matplotlib\", \"seaborn\", \"nltk\", \"rake-nltk\",\n",
    "            \"spacy\", \"shapely\", \"geopandas\", \"sklearn\", \"wordcloud\",\n",
    "           \"plotly\", \"wordcloud\",\"contextily\", \"scipy\", \"scikitplot\"]\n",
    "\n",
    "versions = [np.__version__, pd.__version__, matplotlib.__version__,\n",
    "            sns.__version__, nltk.__version__, \"1.0.4\", spacy.__version__,\n",
    "            spacy.__version__, shapely.__version__, gpd.__version__, \n",
    "            sklearn.__version__, plotly.__version__, \n",
    "            wordcloud.__version__, ctx.__version__, scipy.__version__,\n",
    "            skplt.__version__]\n",
    "\n",
    "\n",
    "print(pd.DataFrame.from_dict({\"Package\": packages,\n",
    "                        \"Version\": versions}\n",
    "                      ).set_index(\"Package\").to_latex(index=True))"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "684px",
    "width": "383px"
   },
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "677.847px",
    "left": "1606px",
    "right": "20px",
    "top": "164px",
    "width": "277.969px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
